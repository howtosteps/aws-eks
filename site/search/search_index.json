{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>This tutorial describes how-to-steps to setup a simple EKS cluster. This tutorial will provide steps for following: </p> <ul> <li>How to setup a simple kubernetes cluster using eksctl </li> <li>How to add new nodegroups</li> <li>How to add autoscaler to dynamically scale in and out </li> <li>How to setup a stateless application</li> </ul>"},{"location":"#application-structure","title":"Application Structure","text":"<pre><code>  |\u2500\u2500 docs                                  # Folder contains all the mkdocs files\n  |    \u251c\u2500\u2500 img                              # Contains all images referenced in mkdocs\n  |    \u251c\u2500\u2500 *.md                             # Other mkdocs .md files\n  \u251c\u2500\u2500 mkdocs.yml                            # Yaml for for mkdocs\n  \u251c\u2500\u2500 .gitattributes\n  |\n  |\u2500\u2500 cluster-ng                            # Folder containing a simple EKS cluster with on-demand instances\n  |    \u251c\u2500\u2500 eks-cluster.yaml                 # Yaml file for a simple cluster\n  |    \u251c\u2500\u2500 eks-cluster-cmds.md              # List of commands to run \n  |\u2500\u2500 cluster-ng-mixed                      # Folder containing an EKS cluster with spot &amp; on-demand instances\n  |    \u251c\u2500\u2500 eks-cluster.yaml                 # Yaml file for cluster with different node groups\n  |\u2500\u2500 cluster-autoscaler                    # Folder containing a EKS cluster with auto-scaler\n  |    \u251c\u2500\u2500 eks-cluster.yaml                 # Yaml file for a simple cluster\n  |    \u251c\u2500\u2500 eks-cluster-cmds.md              # List of commands to run \n  |\u2500\u2500 cluster-stateless-app                 # Folder containing a simple stateless example from kubernetes.io\n  |    \u251c\u2500\u2500 frontend-deployment.yaml         # Yaml file for frontend deployment\n  |    \u251c\u2500\u2500 frontend-service.yaml            # Yaml file for frontend service\n  |    \u251c\u2500\u2500 redis-leader-deployment.yaml     # Yaml file for frontend deployment\n  |    \u251c\u2500\u2500 redis-leader-service.yaml        # Yaml file for frontend service\n  |    \u251c\u2500\u2500 redis-followers-deployment.yaml  # Yaml file for frontend deployment\n  |    \u251c\u2500\u2500 redis-follower-service.yaml      # Yaml file for frontend service\n  |    \u251c\u2500\u2500 cluster-stateless-app-cmds.md    # List of commands to run\n  \u251c\u2500\u2500 README.md                             # Standard README.md file\n</code></pre>"},{"location":"#credits","title":"Credits","text":"<p>Although much of the material has been sources from various sources on the web ( including ChatGPT ), I did find this Udemy tutorial very useful in crafting these how-to-steps :</p> <p>Amazon EKS Starter: Docker on AWS EKS with Kubernetes</p>"},{"location":"clean-up/","title":"Clean up","text":""},{"location":"clean-up/#cleaning-up","title":"Cleaning Up","text":"<p>For clean up, all applications are undeployed using </p> <pre><code>kubectl delete -f &lt;app-deployment-file&gt;\nkubectl delete -f &lt;app-service-file&gt;\n</code></pre> <p>Get all nodegroups</p> <pre><code>ksctl get nodegroup --cluster=eks-cluster-ng\n</code></pre> <pre><code>PS C:\\Users\\aniru\\workspace\\github\\aws-eks\\cluster-stateless-app&gt; eksctl get nodegroup --cluster=eks-cluster-ngCLUSTER         NODEGROUP       STATUS          CREATED                 MIN SIZE        MAX SIZE        DESIRED CAPACITY        INSTANCE TYPE   IMAGE ID           ASG NAME                                                                TYPE\neks-cluster-ng  scale-east1b    CREATE_COMPLETE 2023-02-14T12:10:31Z    1               2               1                       t2.nano         ami-0d4bdb1cf2f07d811      eksctl-eks-cluster-ng-nodegroup-scale-east1b-NodeGroup-PVRF5CRPR3DU     unmanaged\neks-cluster-ng  scale-east1c    CREATE_COMPLETE 2023-02-14T14:39:47Z    1               3               3                       t2.micro        ami-0d4bdb1cf2f07d811      eksctl-eks-cluster-ng-nodegroup-scale-east1c-NodeGroup-K1ALBP32WELN     unmanaged\neks-cluster-ng  scale-spot      CREATE_COMPLETE 2023-02-14T12:10:32Z    1               4               2                       t2.micro        ami-0d4bdb1cf2f07d811      eksctl-eks-cluster-ng-nodegroup-scale-spot-NodeGroup-1SZS5D6OYGK21      unmanaged\n</code></pre> <p>First delete all nodegroups</p> <pre><code>eksctl delete nodegroup --cluster=eks-cluster-ng --name=scale-east1b\neksctl delete nodegroup --cluster=eks-cluster-ng --name=scale-east1c\neksctl delete nodegroup --cluster=eks-cluster-ng --name=scale-spot\n</code></pre> <p>Now delete the cluster</p> <pre><code>eksctl delete cluster --name=eks-cluster-ng\n</code></pre>"},{"location":"create-cluster-autoscaler/","title":"Create cluster with autoscaler","text":"<p>In this section, we will create a autoscaler that allows us to dynamically scale the nodes within a nodegroup - in and out.  We will start by adding 3 new nodegroups to our existing cluster <code>eks-cluster-ng</code>: </p> <ul> <li>scale-east1b : Represents on-demand instances of one instance type running on a single AZ for stateful workloads</li> <li>scale-east1c : Represents on-demand instances of multiple instance types running on a single AZ for stateful workloads</li> <li>scale-spot : represents spot instances running on multi AZ for stateless workloads</li> </ul> <p>The cluster auto-scaler automatically launches additional worker nodes if more resources are needed, and shutdowns worker nodes if they are under-utilized. The autoscaling works within a nodegroup, hence we will create a nodegroup first which has this feature enabled.</p> <p>Create folder <code>cluster-autoscaler</code>. Copy the following contents into file  <code>eks-cluster.yaml</code>:</p> <pre><code>piVersion: eksctl.io/v1alpha5\nkind: ClusterConfig\n\nmetadata:\n  name: eks-cluster-ng\n  region: us-east-1\n\nnodeGroups:\n  - name: scale-east1b\n    instanceType: t2.nano\n    desiredCapacity: 1\n    maxSize: 2\n    availabilityZones: [\"us-east-1b\"]\n    iam:\n      withAddonPolicies:\n        autoScaler: true\n    labels:\n      nodegroup-type: stateful-east1b\n      instance-type: onDemand\n  - name: scale-east1c\n    instanceType: t2.micro\n    desiredCapacity: 1\n    maxSize: 3\n    availabilityZones: [\"us-east-1c\"]\n    iam:\n      withAddonPolicies:\n        autoScaler: true\n    labels:\n      nodegroup-type: stateful-east1c\n      instance-type: onDemand\n    ssh: # use existing EC2 key\n      publicKeyName: eks-course\n  - name: scale-spot\n    desiredCapacity: 1\n    maxSize: 4\n    instancesDistribution:\n      instanceTypes: [\"t2.micro\", \"t2.small\"]\n      onDemandBaseCapacity: 0\n      onDemandPercentageAboveBaseCapacity: 0\n    availabilityZones: [\"us-east-1b\",\"us-east-1c\"]\n    iam:\n      withAddonPolicies:\n        autoScaler: true\n    labels:\n      nodegroup-type: stateless-workload\n      instance-type: spot\n    ssh: \n      publicKeyName: eks-course\n\navailabilityZones: [\"us-east-1b\",\"us-east-1c\"]\n</code></pre> <p>This will tell EKS to :</p> <ul> <li>Apply additional cluster configurations to cluster <code>eks-cluster-ng</code></li> <li>Apply defintions for 3 nodegroups :<ul> <li>scale-east1b : On-demand instances in us-east-1b</li> <li>scale-east1c : On-demand instances in us-east-1c</li> <li>scale-spot : Spot instances in us-east-1b, us-east-1c</li> </ul> </li> <li>Note the autoScaler property is set to <code>true</code></li> </ul>"},{"location":"create-cluster-autoscaler/#create-cluster-nodegroups","title":"Create cluster nodegroups","text":"<p>We will now create the new nodegroups in our existing cluster</p> <pre><code>eksctl create nodegroup --config-file=eks-cluster.yaml\n</code></pre> <pre><code>kubectl get nodes\n</code></pre> <pre><code>PS C:\\Users\\aniru\\workspace\\github\\aws-eks\\cluster-autoscaler&gt; kubectl get nodes\nNAME                             STATUS   ROLES    AGE     VERSION\nip-192-168-14-220.ec2.internal   Ready    &lt;none&gt;   2m7s    v1.23.15-eks-49d8fe8\nip-192-168-20-253.ec2.internal   Ready    &lt;none&gt;   4h51m   v1.23.15-eks-49d8fe8\nip-192-168-33-152.ec2.internal   Ready    &lt;none&gt;   4h51m   v1.23.15-eks-49d8fe8\nip-192-168-53-222.ec2.internal   Ready    &lt;none&gt;   78s     v1.23.15-eks-49d8fe8\nip-192-168-61-217.ec2.internal   Ready    &lt;none&gt;   2m56s   v1.23.15-eks-49d8fe8\n</code></pre> <p>Check the AGE column to see which of the instances were started. Now let's check the nodegroups using :</p> <pre><code>eksctl get nodegroup --cluster=eks-cluster-ng\n</code></pre> <pre><code>PS C:\\Users\\aniru\\workspace\\github\\aws-eks\\cluster-autoscaler&gt; eksctl get nodegroup --cluster=eks-cluster-ng\nCLUSTER         NODEGROUP       STATUS          CREATED                 MIN SIZE        MAX SIZE        DESIRED CAPACITY        INSTANCE TYPE   IMAGE ID           ASG NAME                                                                TYPE\neks-cluster-ng  ng-1            CREATE_COMPLETE 2023-02-14T07:23:00Z    2               2               2                       t2.nano         ami-0d4bdb1cf2f07d811      eksctl-eks-cluster-ng-nodegroup-ng-1-NodeGroup-1LRT8H4ZKAVO9            unmanaged\neks-cluster-ng  scale-east1b    CREATE_COMPLETE 2023-02-14T12:10:31Z    1               2               1                       t2.nano         ami-0d4bdb1cf2f07d811      eksctl-eks-cluster-ng-nodegroup-scale-east1b-NodeGroup-PVRF5CRPR3DU     unmanaged\neks-cluster-ng  scale-east1c    CREATE_COMPLETE 2023-02-14T12:10:31Z    1               3               1                       t2.nano         ami-0d4bdb1cf2f07d811      eksctl-eks-cluster-ng-nodegroup-scale-east1c-NodeGroup-15I9RHLLNMPQR    unmanaged\neks-cluster-ng  scale-spot      CREATE_COMPLETE 2023-02-14T12:10:32Z    1               4               1                       t2.micro        ami-0d4bdb1cf2f07d811      eksctl-eks-cluster-ng-nodegroup-scale-spot-NodeGroup-1SZS5D6OYGK21      unmanaged\n</code></pre> <p>Let's delete the starter nodegroup <code>ng-1</code></p> <pre><code>eksctl delete nodegroup --cluster=eks-cluster-ng  --name='ng-1'          \n</code></pre> <pre><code>PS C:\\Users\\aniru\\workspace\\github\\aws-eks\\cluster-autoscaler&gt; eksctl delete nodegroup --cluster=eks-cluster-ng  --name='ng-1'\n2023-02-14 07:31:27 [\u2139]  1 nodegroup (ng-1) was included (based on the include/exclude rules)\n2023-02-14 07:31:28 [\u2139]  will drain 1 nodegroup(s) in cluster \"eks-cluster-ng\"\n2023-02-14 07:31:28 [\u2139]  starting parallel draining, max in-flight of 1\n2023-02-14 07:31:31 [\u2139]  cordon node \"ip-192-168-20-253.ec2.internal\"\n2023-02-14 07:31:31 [\u2139]  cordon node \"ip-192-168-33-152.ec2.internal\"\n</code></pre>"},{"location":"create-cluster-autoscaler/#create-deployment-for-auto-scaler","title":"Create deployment for auto-scaler","text":"<p>Deploy the auto-scaler itself:</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/kubernetes/autoscaler/master/cluster-autoscaler/cloudprovider/aws/examples/cluster-autoscaler-autodiscover.yaml\n</code></pre> <pre><code>PS C:\\Users\\aniru\\workspace\\github\\aws-eks\\cluster-autoscaler&gt; kubectl apply -f https://raw.githubusercontent.com/kubernetes/autoscaler/master/cluster-autoscaler/cloudprovider/aws/examples/cluster-autoscaler-autodiscover.yaml\nserviceaccount/cluster-autoscaler created\nclusterrole.rbac.authorization.k8s.io/cluster-autoscaler created\nrole.rbac.authorization.k8s.io/cluster-autoscaler created\nclusterrolebinding.rbac.authorization.k8s.io/cluster-autoscaler created\nrolebinding.rbac.authorization.k8s.io/cluster-autoscaler created\ndeployment.apps/cluster-autoscaler created\n</code></pre>"},{"location":"create-cluster-autoscaler/#add-annotation-to-the-deployment","title":"Add annotation to the deployment","text":"<p>This prevents from being evicted</p> <pre><code>kubectl -n kube-system annotate deployment.apps/cluster-autoscaler cluster-autoscaler.kubernetes.io/safe-to-evict=\"false\"\n</code></pre> <pre><code>PS C:\\Users\\aniru\\workspace\\github\\aws-eks\\cluster-autoscaler&gt; kubectl -n kube-system annotate deployment.apps/cluster-autoscaler cluster-autoscaler.kubernetes.io/safe-to-evict=\"false\"\ndeployment.apps/cluster-autoscaler annotated\n</code></pre>"},{"location":"create-cluster-autoscaler/#set-image-version-and-cluster-name","title":"Set image version and cluster name","text":"<p>We will now set matching image version and cluster name <code>eks-cluster-ng</code> in the deployment. Get the autoscaler image version:  </p> <p>Open Kubernetes/Autoscalar Releases and get the latest release version matching your Kubernetes version, e.g. Kubernetes 1.14 =&gt; check for 1.14.n where \"n\" is the latest release version</p> <p></p> <p>Edit deployment and set your EKS cluster name:</p> <pre><code>kubectl -n kube-system edit deployment.apps/cluster-autoscaler\n</code></pre> <ul> <li>set the image version at property <code>image=k8s.gcr.io/cluster-autoscaler:vx.yy.z</code> </li> </ul> <pre><code>image: registry.k8s.io/autoscaling/cluster-autoscaler:v1.23.0\n</code></pre> <ul> <li>set your EKS cluster name at the end of property <code>- --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled,k8s.io/cluster-autoscaler/&lt;&lt;EKS cluster name&gt;&gt;</code></li> </ul> <pre><code>- --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled,k8s.io/cluster-autoscaler/eks-cluster-ng\n</code></pre> <pre><code>kubectl -n kube-system describe deployment cluster-autoscaler\n</code></pre>"},{"location":"create-cluster/","title":"Create simple Cluster","text":"<p>In this section, we will create a simple cluster using <code>eksctl</code></p>"},{"location":"create-cluster/#create-yaml-file","title":"Create Yaml file","text":"<p>Create folder <code>cluster-ng</code>. The first step is to create a simple yaml file in this folder. Let's call it <code>eks-cluster.yaml</code>. Copy the following contents to it:  </p> <pre><code>apiVersion: eksctl.io/v1alpha5\nkind: ClusterConfig\n\nmetadata:\n  name: eks-cluster-ng\n  region: us-east-1\n\nnodeGroups:\n  - name: ng-1\n    instanceType: t2.nano\n    desiredCapacity: 2\n    ssh: # use existing EC2 key\n      publicKeyName: eks-course\n</code></pre> <p>This tells EKS to:</p> <ul> <li>Create a cluster <code>eks-cluster-ng</code> in region us-east-1</li> <li>Call the name of the nodegroup : ng-1</li> <li>Set the instance type as :t2.micro</li> <li>Set the desired capcity of instances as 2</li> <li>Set ssh key : eks-course </li> </ul>"},{"location":"create-cluster/#create-cluster","title":"Create cluster","text":"<p>Now we will create the cluster by specfying the file to use with flag <code>f</code> with command : <code>eksctl create cluster</code></p> <pre><code>eksctl create cluster -f eks-cluster.yaml\n</code></pre> <p>This will take 10-15 minutes. Now log into AWS console and navigate to the Cloud Formation service. You will see that <code>eksctl</code> creates a cloud formation stack. Check the resources tab to glace through various resources being created. </p> <p></p>"},{"location":"create-cluster/#check-cluster","title":"Check cluster","text":"<p>Now let's check the EC2 console. You will see 2 instances :</p> <p></p> <p>You can also see VPCs &amp; Security groups created by cloud formation service : </p> <p>Let's use the <code>eksctl get nodegroup</code> command to get the details of the nodegroup we just created  : </p> <pre><code>eksctl get nodegroup --cluster eks-cluster-ng\n</code></pre> <pre><code>S C:\\Users\\aniru\\workspace\\github\\aws-eks&gt; eksctl get nodegroup --cluster eks-cluster-ng\nCLUSTER         NODEGROUP       STATUS          CREATED                 MIN SIZE        MAX SIZE        DESIRED CAPACITY        INSTANCE TYPE   IMAGE ID                ASG NAME                                       TYPE\neks-cluster-ng  ng-1            CREATE_COMPLETE 2023-01-25T00:22:41Z    2               2               2                       t2.nano         ami-03a30cc1dda93f173   eksctl-eks-cluster-ng-nodegroup-ng-1-NodeGroup-6387ICJKD3BS     unmanaged\n</code></pre> <p>Let's use the <code>kubectl</code> command to get the nodes : </p> <pre><code>kubectl get nodes\n</code></pre> <pre><code>PS C:\\Users\\aniru\\workspace\\github\\aws-eks&gt; kubectl get nodes\nKubeconfig user entry is using deprecated API version client.authentication.k8s.io/v1alpha1. Run 'aws eks update-kubeconfig' to update.\nNAME                             STATUS   ROLES    AGE   VERSION\nip-192-168-21-105.ec2.internal   Ready    &lt;none&gt;   71m   v1.22.15-eks-fb459a0\nip-192-168-54-187.ec2.internal   Ready    &lt;none&gt;   71m   v1.22.15-eks-fb459a0\n</code></pre>"},{"location":"create-stateless-app/","title":"Create Stateless App","text":"<p>In this section we will :</p> <ul> <li>Deploy backend resources</li> <li>Deploy frontend resources</li> <li>Scale pods up/down </li> <li>Perform chaos testing</li> </ul>"},{"location":"create-stateless-app/#setup-sample-guestbook-app","title":"Setup sample guestbook app","text":"<p>This example is based on Kubernetes Guestbook. The application consists of:</p> <ul> <li>A Redis backend with a single leader pod for Writes &amp; multiple follower pods for Reads</li> <li>A frontend app ( Guestbook app ) in PHP loadbalanced to public using a load balancer</li> <li>Guestbook app reads load balanced to multiple redis followers (reads)</li> <li>Guestbook app writes directed towards single redis leader (writes)</li> </ul> <p></p>"},{"location":"create-stateless-app/#backend-deployment","title":"Backend Deployment","text":"<p>Our configuration files for Redis leader/followers includes both Deployment and Services kinds. Deployments and Services are often used in tandem: Deployments working to define the desired state of the application and Services working to make sure communication between almost any kind of resource and the rest of the cluster is stable and adaptable. </p> <p>For the purpose of this application we will scale our nodegroup <code>scale-spot</code> to 3 instances</p> <pre><code>PS C:\\Users\\aniru\\workspace\\github\\aws-eks\\cluster-stateless-app&gt; eksctl scale nodegroup --cluster=eks-cluster-ng --nodes=3 --nodes-max=4 --name=scale-spot\n2023-02-14 10:22:58 [\u2139]  scaling nodegroup \"scale-spot\" in cluster eks-cluster-ng\n2023-02-14 10:23:02 [\u2139]  nodegroup successfully scaled\n</code></pre>"},{"location":"create-stateless-app/#redis-leader","title":"Redis leader","text":"<p>For the Redis leader, first create the Deployment file <code>redis-leader-deployment.yaml</code>. Copy these contents to it : </p> <pre><code># SOURCE: https://cloud.google.com/kubernetes-engine/docs/tutorials/guestbook\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis-leader\n  labels:\n    app: redis\n    role: leader\n    tier: backend\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: redis\n  template:\n    metadata:\n      labels:\n        app: redis\n        role: leader\n        tier: backend\n    spec:\n      containers:\n      - name: leader\n        image: \"docker.io/redis:6.0.5\"\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        ports:\n        - containerPort: 6379\n</code></pre> <p>This tells EKS :</p> <ul> <li>The deployment has replica of 1</li> <li>Specifies the source of image </li> <li>Set resource requirement for cpu &amp; memory</li> <li>Set the container port to 6379</li> </ul> <p>Now let's create the Service on top of your deployment. Create file <code>redis-leader-service.yaml</code> and copy the following contents to it:</p> <pre><code># SOURCE: https://cloud.google.com/kubernetes-engine/docs/tutorials/guestbook\napiVersion: v1\nkind: Service\nmetadata:\n  name: redis-leader\n  labels:\n    app: redis\n    role: leader\n    tier: backend\nspec:\n  ports:\n  - port: 6379\n    targetPort: 6379\n  selector:\n    app: redis\n    role: leader\n    tier: backend\n</code></pre> <p>Notice that the metadata name and labels are same as the redis-leader Deployment. Here the target port that is exposed is set to 6379. </p> <p>Now that the configuration files are defined, let's deploy the Redis leader pod :</p> <pre><code>kubectl apply -f redis-leader-deployment.yaml\n</code></pre> <p>Let's test by checking if the pods were created</p> <pre><code>kubectl get pods\n</code></pre> <pre><code>PS C:\\Users\\aniru\\workspace\\github\\aws-eks\\cluster-stateless-app&gt; kubectl get pod \nNAME                               READY   STATUS    RESTARTS   AGE\nredis-leader-766465cd9c-wh94g      1/1     Running   0          46s\ntest-autoscaler-6545fc8df4-7pktn   0/1     Pending   0          66m\n</code></pre> <p>Now we apply Redis service on top of this :</p> <pre><code>kubectl apply -f redis-leader-service.yaml\n</code></pre> <p>Let's test using :</p> <pre><code>kubectl get service redis-leader\n</code></pre> <pre><code>PS C:\\Users\\aniru\\workspace\\github\\aws-eks\\cluster-stateless-app&gt; kubectl get service redis-leader\nNAME           TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE\nredis-leader   ClusterIP   10.100.232.230   &lt;none&gt;        6379/TCP   11s\n</code></pre>"},{"location":"create-stateless-app/#redis-follower","title":"Redis follower","text":"<p>For the Redis follower, let's create the Deployment file <code>redis-followers-deployment.yaml</code>. Copy these contents to it : </p> <pre><code># SOURCE: https://cloud.google.com/kubernetes-engine/docs/tutorials/guestbook\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis-follower\n  labels:\n    app: redis\n    role: follower\n    tier: backend\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: redis\n  template:\n    metadata:\n      labels:\n        app: redis\n        role: follower\n        tier: backend\n    spec:\n      containers:\n      - name: follower\n        image: gcr.io/google_samples/gb-redis-follower:v2\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        ports:\n        - containerPort: 6379\n</code></pre> <p>Just like before we will now create the Service file as <code>redis-follower-service.yaml</code>. Copy the following contents to it : </p> <pre><code># SOURCE: https://cloud.google.com/kubernetes-engine/docs/tutorials/guestbook\napiVersion: v1\nkind: Service\nmetadata:\n  name: redis-follower\n  labels:\n    app: redis\n    role: follower\n    tier: backend\nspec:\n  ports:\n    # the port that this service should serve on\n  - port: 6379\n  selector:\n    app: redis\n    role: follower\n    tier: backend\n</code></pre> <p>Notice the labels that match the Deployment file and the internal port that the redis follower service should run on. </p> <p>Now that the configuration files are defined, let's deploy the Redis follower pod :</p> <pre><code>kubectl apply -f redis-followers-deployment.yaml\nkubectl apply -f redis-follower-service.yaml\n</code></pre> <p>Let's check the status of Redis followers by running : </p> <pre><code>kubectl get pods -o wide\n</code></pre> <pre><code>kubectl get service\n</code></pre> <pre><code>PS C:\\Users\\aniru\\workspace\\github\\aws-eks\\cluster-stateless-app&gt; kubectl get service\nNAME             TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE\nkubernetes       ClusterIP   10.100.0.1       &lt;none&gt;        443/TCP    7h40m\nredis-follower   ClusterIP   10.100.249.155   &lt;none&gt;        6379/TCP   41m\nredis-leader     ClusterIP   10.100.232.230   &lt;none&gt;        6379/TCP   42m\n</code></pre> <pre><code>kubectl describe node &lt;node-name&gt;\n</code></pre>"},{"location":"create-stateless-app/#frontend-app","title":"Frontend app","text":"<p>Now we will deploy the frontend app that will consist of </p> <ul> <li>3 replicas of the Guestbook App</li> <li>ELB LoadBalancer service</li> </ul>"},{"location":"create-stateless-app/#guestbook-deployment","title":"Guestbook deployment","text":"<p>We will start with defining the Deployment file for the frontend app. Create <code>frontend-deployment.yaml</code> and copy the following contents : </p> <pre><code># SOURCE: https://cloud.google.com/kubernetes-engine/docs/tutorials/guestbook\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: frontend\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n        app: guestbook\n        tier: frontend\n  template:\n    metadata:\n      labels:\n        app: guestbook\n        tier: frontend\n    spec:\n      containers:\n      - name: php-redis\n        image: gcr.io/google_samples/gb-frontend:v5\n        env:\n        - name: GET_HOSTS_FROM\n          value: \"dns\"\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        ports:\n        - containerPort: 80\n</code></pre> <p>This tells EKS to :</p> <ul> <li>Generate 3 replicas of the frontend app</li> <li>Specfiy image source</li> <li>Set the container resources</li> <li>Set the container port</li> </ul> <p>Let's apply the deployment </p> <pre><code>kubectl apply -f frontend-deployment.yaml\n</code></pre> <p>Check all the pods for the guestbook app</p> <pre><code>kubectl get pods\nkubectl get pods -l app=guestbook\nkubectl get pods -l app=guestbook -l tier=frontend\n</code></pre> <pre><code>PS C:\\Users\\aniru\\workspace\\github\\aws-eks\\cluster-stateless-app&gt; kubectl get pods -l app=guestbook -l tier=frontend\nNAME                        READY   STATUS    RESTARTS   AGE\nfrontend-57df59b89c-5rkzr   1/1     Running   0          28m\nfrontend-57df59b89c-f26lm   1/1     Running   0          28m\nfrontend-57df59b89c-s7n2s   1/1     Running   0          28m\n</code></pre>"},{"location":"create-stateless-app/#loadbalancer-service","title":"Loadbalancer service","text":"<p>We will now apply the loadbalancer service that allows us to expose all the loadbalanced service to the public. </p> <p>Create file <code>frontend-service.yaml</code> and copy the following : </p> <pre><code># SOURCE: https://cloud.google.com/kubernetes-engine/docs/tutorials/guestbook\napiVersion: v1\nkind: Service\nmetadata:\n  name: frontend\n  labels:\n    app: guestbook\n    tier: frontend\nspec:\n  # if your cluster supports it, uncomment the following to automatically create\n  # an external load-balanced IP for the frontend service.\n  # type: LoadBalancer\n  type: LoadBalancer\n  ports:\n    # the port that this service should serve on\n  - port: 80\n  selector:\n    app: guestbook\n    tier: frontend\n</code></pre> <p>Let's apply the loadbalanced service</p> <pre><code>kubectl apply -f frontend-service.yaml\n</code></pre> <p>Check all services</p> <pre><code>kubectl get services\nkubectl get service frontend \nkubetctl get pods -o wide\nkubectl describe service frontend\n</code></pre> <pre><code>PS C:\\Users\\aniru\\workspace\\github\\aws-eks\\cluster-stateless-app&gt; kubectl get service frontend\nNAME       TYPE           CLUSTER-IP       EXTERNAL-IP                                                               PORT(S)        AGE\nfrontend   LoadBalancer   10.100.141.207   ab8f47054a1bb400f8700134e4410f1c-1044573003.us-east-1.elb.amazonaws.com   80:30074/TCP   91s\n</code></pre> <p>Finally check AWS mgmt console for the ELB which has been created !</p>"},{"location":"create-stateless-app/#access-from-outside-the-cluster","title":"Access from outside the cluster","text":"<p>Grab the public DNS of the frontend service LoadBalancer (ELB):</p> <pre><code>kubectl describe service frontend\n</code></pre> <p>Copy the name and paste it into your browser !!!</p>"},{"location":"create-stateless-app/#scale-pods-up-and-down","title":"Scale Pods up and down","text":"<p>We can scale pods up and down in the following ways:</p> <ul> <li>kubectl</li> <li>Update YAML file</li> </ul>"},{"location":"create-stateless-app/#using-kubectl","title":"Using kubectl","text":"<p>We can scale up and down using kubectl command. To scale up run the following command :  </p> <pre><code>kubectl scale deployment frontend --replicas=5\n</code></pre> <pre><code>PS C:\\Users\\aniru\\workspace\\github\\aws-eks\\cluster-stateless-app&gt; kubectl get pods\nNAME                               READY   STATUS    RESTARTS   AGE\nfrontend-57df59b89c-5rkzr          1/1     Running   0          37m\nfrontend-57df59b89c-f26lm          1/1     Running   0          37m\nfrontend-57df59b89c-rhwrv          0/1     Pending   0          5s\nfrontend-57df59b89c-s7n2s          1/1     Running   0          37m\nfrontend-57df59b89c-xxjll          1/1     Running   0          5s\nredis-follower-84fcc94dfc-2k2pn    1/1     Running   0          91m\nredis-follower-84fcc94dfc-k6d2t    1/1     Running   0          91m\nredis-leader-766465cd9c-wh94g      1/1     Running   0          93m\ntest-autoscaler-59c78f9974-7jn9n   1/1     Running   0          76m\n</code></pre> <p>Let's check the pods now</p> <pre><code>kubectl get pods\n</code></pre> <p>Now let us scale it down</p> <pre><code>kubectl scale deployment frontend --replicas=3\n</code></pre> <pre><code>PS C:\\Users\\aniru\\workspace\\github\\aws-eks\\cluster-stateless-app&gt; kubectl get pods\nNAME                               READY   STATUS    RESTARTS   AGE\nfrontend-57df59b89c-5rkzr          1/1     Running   0          38m\nfrontend-57df59b89c-f26lm          1/1     Running   0          38m\nfrontend-57df59b89c-s7n2s          1/1     Running   0          38m\nredis-follower-84fcc94dfc-2k2pn    1/1     Running   0          92m\nredis-follower-84fcc94dfc-k6d2t    1/1     Running   0          92m\nredis-leader-766465cd9c-wh94g      1/1     Running   0          94m\ntest-autoscaler-59c78f9974-7jn9n   1/1     Running   0          76m\n</code></pre> <p>You can check pods are terminating by checking the pods</p> <pre><code>kubectl get pods\n</code></pre>"},{"location":"create-stateless-app/#using-yaml-file","title":"Using YAML file","text":"<p>We can scale the pods by updating the replicas tag in our yaml files. Let's test this out by updating file <code>redis-followers-deployment.yaml</code>. Update the replicas count to 5 as follows :</p> <pre><code># SOURCE: https://cloud.google.com/kubernetes-engine/docs/tutorials/guestbook\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: frontend\nspec:\n  replicas: 5\n  selector:\n    matchLabels:\n        app: guestbook\n        tier: frontend\n  template:\n    metadata:\n      labels:\n        app: guestbook\n        tier: frontend\n    spec:\n      containers:\n      - name: php-redis\n        image: gcr.io/google_samples/gb-frontend:v5\n        env:\n        - name: GET_HOSTS_FROM\n          value: \"dns\"\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        ports:\n        - containerPort: 80\n</code></pre> <p>Let's apply the new deployment file</p> <pre><code>kubectl apply -f redis-followers-deployment.yaml\n</code></pre> <p>Let's check this by running</p> <pre><code>kubectl get deployment redis-follower\nkubectl get pods\n</code></pre> <pre><code>PS C:\\Users\\aniru\\workspace\\github\\aws-eks\\cluster-stateless-app&gt; kubectl get deployment redis-follower\nNAME             READY   UP-TO-DATE   AVAILABLE   AGE\nredis-follower   5/5     5            5           99m\n</code></pre> <pre><code>PS C:\\Users\\aniru\\workspace\\github\\aws-eks\\cluster-stateless-app&gt; kubectl get pods\nNAME                               READY   STATUS    RESTARTS   AGE\nfrontend-57df59b89c-5rkzr          1/1     Running   0          42m\nfrontend-57df59b89c-f26lm          1/1     Running   0          42m\nfrontend-57df59b89c-s7n2s          1/1     Running   0          42m\nredis-follower-84fcc94dfc-2k2pn    1/1     Running   0          95m\nredis-follower-84fcc94dfc-5fvt6    1/1     Running   0          31s\nredis-follower-84fcc94dfc-k6d2t    1/1     Running   0          95m\nredis-follower-84fcc94dfc-s6mcd    1/1     Running   0          31s\nredis-follower-84fcc94dfc-v4bcx    1/1     Running   0          31s\nredis-leader-766465cd9c-wh94g      1/1     Running   0          98m\ntest-autoscaler-59c78f9974-7jn9n   1/1     Running   0          80m\n</code></pre>"},{"location":"create-stateless-app/#perform-chaos-testing","title":"Perform chaos testing","text":"<p>We will demonstrate self-healing mechanism of K8s by:</p> <ul> <li>Kill pods</li> <li>Stop K8s worker node(s)</li> </ul> <pre><code>kubectl get pods\nkubectl get nodes\n</code></pre> <pre><code>kubectl get pods -o wide\n</code></pre> <p>Now let's delete one of front-end pods</p> <pre><code>kubectl delete pod &lt;frontend-pod&gt;\n</code></pre> <p>Now check for pods look a the age column. Kubernetes restarts the pod</p> <pre><code>kubectl get pods -o wide\n</code></pre> <pre><code>PS C:\\Users\\aniru\\workspace\\github\\aws-eks\\cluster-stateless-app&gt; kubectl delete pod redis-follower-84fcc94dfc-k6d2t\npod \"redis-follower-84fcc94dfc-k6d2t\" deleted\nPS C:\\Users\\aniru\\workspace\\github\\aws-eks\\cluster-stateless-app&gt; kubectl get pods\nNAME                               READY   STATUS    RESTARTS   AGE\nfrontend-57df59b89c-5rkzr          1/1     Running   0          54m\nfrontend-57df59b89c-f26lm          1/1     Running   0          54m\nfrontend-57df59b89c-s7n2s          1/1     Running   0          54m\nredis-follower-84fcc94dfc-2k2pn    1/1     Running   0          107m\nredis-follower-84fcc94dfc-4wn4v    1/1     Running   0          87s\nredis-follower-84fcc94dfc-5fvt6    1/1     Running   0          12m\nredis-follower-84fcc94dfc-s6mcd    1/1     Running   0          12m\nredis-follower-84fcc94dfc-v4bcx    1/1     Running   0          12m\nredis-leader-766465cd9c-wh94g      1/1     Running   0          110m\ntest-autoscaler-59c78f9974-7jn9n   1/1     Running   0          92m\n</code></pre> <p>Now let's use AWS console to  stop any instance ( AWS &gt; EC2 &gt; instances). You will notice 2 things :</p> <ul> <li>Kubernetes will try to recover the nodes</li> <li>Kubernetes will reshuffle the nodes if required </li> </ul>"},{"location":"create-stateless-app/#cleaning-up","title":"Cleaning up","text":"<p>Run the following commands to save costs</p> <pre><code>kubectl delete deployment -l app=redis\nkubectl delete service -l app=redis\nkubectl delete deployment frontend\nkubectl delete service frontend\n</code></pre> <pre><code>PS C:\\Users\\aniru\\workspace\\github\\aws-eks\\cluster-stateless-app&gt; kubectl get pods\nNAME                               READY   STATUS    RESTARTS   AGE\ntest-autoscaler-59c78f9974-7jn9n   1/1     Running   0          97m\n</code></pre>"},{"location":"extend-cluster/","title":"Extend Cluster","text":"<p>In this section we will extend the cluster by adding a nodegroup that contains a mix of:</p> <ul> <li>On-demand instances</li> <li>Spot instances</li> </ul>"},{"location":"extend-cluster/#define-yaml-file","title":"Define Yaml file","text":"<p>Create folder <code>cluster-ng-mixed</code>. We will now extend the cluster by adding a nodegroup to file <code>eks-cluster.yaml</code>. The file adds a new nodegroup <code>ng-mixed</code>. </p> <p>Copy the following contents :  </p> <pre><code>apiVersion: eksctl.io/v1alpha5\nkind: ClusterConfig\n\nmetadata:\n  name: eks-cluster-ng\n  region: us-east-1\n\nnodeGroups:\n  - name: ng-1\n    instanceType: t2.nano\n    desiredCapacity: 2\n    ssh: # use existing EC2 key\n      publicKeyName: eks-course\n  - name: ng-mixed\n    minSize: 3\n    maxSize: 5\n    instancesDistribution:\n      maxPrice: 0.2\n      instanceTypes: [\"t2.small\", \"t3.small\"]\n      onDemandBaseCapacity: 0\n      onDemandPercentageAboveBaseCapacity: 50\n    ssh: \n      publicKeyName: eks-course\n\n</code></pre> <p>This tells EKS to : </p> <ul> <li>Add a new nodegroup <code>ng-mixed</code></li> <li>Set min &amp; max size</li> <li>Define the instance types</li> <li>50% of the instance types are on-demand and the rest on spot </li> </ul>"},{"location":"extend-cluster/#add-nodegroup","title":"Add nodegroup","text":"<p>First check your cluster is running :</p> <pre><code>eksctl get cluster\n</code></pre> <pre><code>PS C:\\Users\\aniru\\workspace\\github\\aws-eks\\cluster-ng&gt; eksctl get cluster\nNAME            REGION          EKSCTL CREATED\neks-cluster-ng  us-east-1       True\n</code></pre> <p>Let's add the new nodegroup :</p> <pre><code>eksctl create nodegroup --config-file=eks-cluster.yaml --include='ng-mixed'\n</code></pre> <pre><code>PS C:\\Users\\aniru\\workspace\\github\\aws-eks\\cluster-ng-mixed&gt; eksctl create nodegroup --config-file=eks-cluster.yaml --include='ng-mixed'\n2023-02-14 03:25:59 [\u2139]  will use version 1.23 for new nodegroup(s) based on control plane version\n2023-02-14 03:26:06 [\u2139]  nodegroup \"ng-1\" will use \"ami-0d4bdb1cf2f07d811\" [AmazonLinux2/1.23]\n2023-02-14 03:26:07 [\u2139]  using EC2 key pair \"eks-course\"\n2023-02-14 03:26:07 [\u2139]  nodegroup \"ng-mixed\" will use \"ami-0d4bdb1cf2f07d811\" [AmazonLinux2/1.23]\n2023-02-14 03:26:08 [\u2139]  using EC2 key pair \"eks-course\"\n2023-02-14 03:26:11 [\u2139]  1 existing nodegroup(s) (ng-1) will be excluded\n2023-02-14 03:26:11 [\u2139]  combined include rules: ng-mixed\n2023-02-14 03:26:11 [\u2139]  1 nodegroup (ng-mixed) was included (based on the include/exclude rules)\n2023-02-14 03:26:11 [\u2139]  will create a CloudFormation stack for each of 1 nodegroups in cluster \"eks-cluster-ng\"\n</code></pre> <p>Note line combined include rules: ng-mixed</p> <p>Also check the new nodegroup <code>ng-mixed</code> created after a few minutes by running below command. Check min size and max size  : </p> <pre><code>PS C:\\Users\\aniru\\workspace\\github\\aws-eks&gt; eksctl get nodegroup --cluster=eks-cluster-ng\nCLUSTER         NODEGROUP       STATUS          CREATED                 MIN SIZE        MAX SIZE        DESIRED CAPACITY        INSTANCE TYPE   IMAGE ID           ASG NAME                                                         TYPE\neks-cluster-ng  ng-1            CREATE_COMPLETE 2023-02-14T07:23:00Z    2               2               2                       t2.nano         ami-0d4bdb1cf2f07d811       eksctl-eks-cluster-ng-nodegroup-ng-1-NodeGroup-1LRT8H4ZKAVO9            unmanaged\neks-cluster-ng  ng-mixed        CREATE_COMPLETE 2023-02-14T08:26:13Z    3               5               3                       t2.small        ami-0d4bdb1cf2f07d811       eksctl-eks-cluster-ng-nodegroup-ng-mixed-NodeGroup-1I8YSGYWKO2O5        unmanaged\n</code></pre> <p>You can also review this on AWS console :</p> <p></p>"},{"location":"extend-cluster/#delete-nodegroup","title":"Delete nodegroup","text":"<p>Now let's drain the <code>ng-mixed</code> nodegroup we just created so we don't incur additional costs :</p> <pre><code>eksctl delete nodegroup --config-file=eks-cluster.yaml --include='ng-mixed' --approve\n</code></pre> <p>The draining will usually take some time as is indicated by STATUS=DELETE_IN_PROGRESS:</p> <pre><code>PS C:\\Users\\aniru\\workspace\\github\\aws-eks&gt; eksctl get nodegroup --cluster=eks-cluster-ng\nCLUSTER         NODEGROUP       STATUS                  CREATED                 MIN SIZE        MAX SIZE        DESIRED CAPACITY        INSTANCE TYPE   IMAGE ID   ASG NAME                                                         TYPE\neks-cluster-ng  ng-1            CREATE_COMPLETE         2023-02-14T07:23:00Z    2               2               2                       t2.nano         ami-0d4bdb1cf2f07d811       eksctl-eks-cluster-ng-nodegroup-ng-1-NodeGroup-1LRT8H4ZKAVO9            unmanaged\neks-cluster-ng  ng-mixed        DELETE_IN_PROGRESS      2023-02-14T08:26:13Z    0               0               0                       t2.small        ami-0d4bdb1cf2f07d811       eksctl-eks-cluster-ng-nodegroup-ng-mixed-NodeGroup-1I8YSGYWKO2O5        unmanaged\n</code></pre>"},{"location":"local-environment/","title":"Local Enviornment","text":""},{"location":"local-environment/#configurations","title":"Configurations","text":"<p>Navigate to your AWS credentials file and update the default profile with access-keys of the newly created IAM user</p> <pre><code>PS C:\\Users\\aniru\\.aws&gt; cd ~\\.aws\nPS C:\\Users\\aniru\\.aws&gt; ls  \n    Directory: C:\\Users\\aniru\\.aws\nMode                 LastWriteTime         Length Name\n----                 -------------         ------ ----\n-a----         11/1/2022  10:50 AM             99 config\n-a----         1/24/2023   5:19 PM            461 credentials\n</code></pre> <p>Edit <code>credentials</code> file by updating the <code>[default]</code> entry as follows:</p> <pre><code>[default]\naws_access_key_id=###\naws_secret_access_key=###\nregion=us-east-1\noutput=json\n</code></pre>"},{"location":"local-installations/","title":"Local Installations","text":""},{"location":"local-installations/#install-aws-cli","title":"Install AWS CLI","text":"<ul> <li>Follow the instructions from this page: Installing or updating the latest version of the AWS CLI</li> <li>Test </li> </ul> <pre><code>    PS C:\\Users\\aniru\\.aws&gt; aws --version\n    aws-cli/2.7.4 Python/3.9.11 Windows/10 exe/AMD64 prompt/off\n</code></pre>"},{"location":"local-installations/#install-eksctl","title":"Install eksctl","text":"<ul> <li>Follow the installation instructions from here : eksctl installation</li> <li>Test</li> </ul> <pre><code>    PS C:\\Users\\aniru\\.aws&gt; eksctl version  \n    0.99.0\n</code></pre>"},{"location":"local-installations/#install-kubectl","title":"Install kubectl","text":"<ul> <li>Follow instructions from here: kubectl installation</li> <li>Test </li> </ul> <pre><code>    PS C:\\Users\\aniru\\.aws&gt; kubectl version --client\n    Client Version: version.Info{Major:\"1\", Minor:\"23\", GitVersion:\"v1.23.6\", GitCommit:\"ad3338546da947756e8a88aa6822e9c11e7eac22\", GitTreeState:\"clean\", BuildDate:\"2022-04-14T08:49:13Z\", GoVersion:\"go1.17.9\", Compiler:\"gc\", Platform:\"windows/amd64\"}\n</code></pre>"},{"location":"prerequisites/","title":"Prerequisites","text":""},{"location":"prerequisites/#iam-user-and-permissions","title":"IAM user and permissions","text":"<p>We will create a new IAM user (<code>eks-&lt;user-name&gt;</code>) for the purposes of the tutorial. Please keep in mind that you should never use your root account for working with AWS services. </p>"},{"location":"prerequisites/#create-policies","title":"Create policies","text":"<p>Create these 2 policies:</p> <ul> <li>EKS-Admin-policy:</li> </ul> <pre><code>    {\n        \"Version\": \"2012-10-17\",\n        \"Statement\": [\n            {\n                \"Effect\": \"Allow\",\n                \"Action\": [\n                    \"eks:*\"\n                ],\n                \"Resource\": \"*\"\n            }\n        ]\n    }\n</code></pre> <ul> <li>CloudFormation-Admin-policy:</li> </ul> <pre><code>    {\n        \"Version\": \"2012-10-17\",\n        \"Statement\": [\n            {\n                \"Effect\": \"Allow\",\n                \"Action\": [\n                    \"cloudformation:*\"\n                ],\n                \"Resource\": \"*\"\n            }\n        ]\n    }\n</code></pre>"},{"location":"prerequisites/#assign-policies","title":"Assign policies","text":"<p>Finally, assign the following policies to your IAM user you are going to use throughout the course:</p> <ul> <li>AmazonEC2FullAccess</li> <li>IAMFullAccess</li> <li>AmazonVPCFullAccess</li> <li>CloudFormation-Admin-policy</li> <li>EKS-Admin-policy where the last 2 policies are the ones you created above</li> </ul> <p></p>"},{"location":"prerequisites/#create-iam-role","title":"Create IAM role","text":"<ul> <li>Open <code>https://console.aws.amazon.com/iam/</code> and choose Roles =&gt; create role </li> <li>Choose EKS service followed by Allows Amazon EKS to manage your clusters on your behalf </li> <li>Choose Next: Permissions</li> <li>Click Next: Review</li> <li>Enter a unique Role name, EKS-course-role and click Create Role</li> </ul>"},{"location":"prerequisites/#create-key-pair","title":"Create key Pair","text":"<ul> <li>Open EC2 dashboard <code>https://console.aws.amazon.com/ec2</code></li> <li>Click KeyPairs in left navigation bar under section \"Network&amp;Security\"</li> <li>Click Create Key Pair</li> <li>Provide name for keypair, eks-course and click Create<ul> <li>The keypair will be downloaded immediately =&gt; file eks-course.pem</li> </ul> </li> </ul>"},{"location":"prerequisites/#create-api-access-keysecret","title":"Create API Access key/secret","text":"<ul> <li>Create key+secret via AWS console   AWS-console =&gt; IAM =&gt; Users =&gt;  =&gt; tab Security credentials =&gt; button Create access key"},{"location":"test-cluster-autoscaler/","title":"Test Auto-scaler with Nginx deployment","text":"<p>We will now test the auto-scaler and scale it up and down</p>"},{"location":"test-cluster-autoscaler/#create-the-deployment-file","title":"Create the deployment file","text":"<p>In folder <code>cluster-autoscaler</code>, we will create a new deployment file <code>nginx-deployment.yaml</code>. Copy the following contents to it: </p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: test-autoscaler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        service: nginx\n        app: nginx\n    spec:\n      containers:\n      - image: nginx\n        name: test-autoscaler\n        resources:\n          limits:\n            cpu: 300m\n            memory: 256Mi\n          requests:\n            cpu: 300m\n            memory: 256Mi\n      nodeSelector:\n        instance-type: spot\n</code></pre> <p>This will tell eksctl to :</p> <ul> <li>Refer to this one as of kind: Deployment</li> <li>This will have 1 replica with service: nginx and app: nginx</li> <li>Container will use image: nginx and will call it as name: test-autoscaler</li> <li>Specify resources with cpu &amp; memory tags</li> <li>nodeSelector specifies what kind of instances we will need using tag instance-type: spot</li> </ul>"},{"location":"test-cluster-autoscaler/#deploy-nginx","title":"Deploy Nginx","text":"<p>We will now apply the deployment file using kubectl command</p> <pre><code>kubectl apply -f nginx-deployment.yaml\n</code></pre> <p>Now check if the pod is running</p> <pre><code>kubectl get pod\n</code></pre> <pre><code>S C:\\Users\\aniru\\workspace\\github\\aws-eks\\cluster-autoscaler&gt; kubectl get pod\nNAME                               READY   STATUS    RESTARTS   AGE\ntest-autoscaler-6545fc8df4-7pktn   0/1     Pending   0          22s\n</code></pre> <p>check the instance types </p> <pre><code>kubectl get nodes -l instance-type=spot\n</code></pre> <pre><code>PS C:\\Users\\aniru\\workspace\\github\\aws-eks\\cluster-autoscaler&gt; kubectl get nodes -l instance-type=spot\nNAME                            STATUS   ROLES    AGE    VERSION\nip-192-168-12-53.ec2.internal   Ready    &lt;none&gt;   8m5s   v1.23.15-eks-49d8fe8\n</code></pre>"},{"location":"test-cluster-autoscaler/#scale-the-deployment","title":"Scale the deployment","text":"<p>Let's scale deployment to 3 replicas so we can test the autoscaler</p> <pre><code>kubectl scale --replicas=3 deployment/test-autoscaler\n</code></pre> <pre><code>S C:\\Users\\aniru\\workspace\\github\\aws-eks\\cluster-autoscaler&gt; kubectl scale --replicas=3 deployment/test-autoscaler\ndeployment.apps/test-autoscaler scaled\n</code></pre> <p>You should see 3 pods running now but using the same node </p> <pre><code>kubectl get pod\n</code></pre> <pre><code>S C:\\Users\\aniru\\workspace\\github\\aws-eks\\cluster-autoscaler&gt; kubectl get pod\nNAME                               READY   STATUS    RESTARTS   AGE\ntest-autoscaler-6545fc8df4-6qk5f   0/1     Pending   0          33s\ntest-autoscaler-6545fc8df4-7pktn   0/1     Pending   0          6m12s\ntest-autoscaler-6545fc8df4-hmrxd   0/1     Pending   0          33s\n</code></pre> <pre><code>kubectl get nodes -l instance-type=spot\n</code></pre> <pre><code>PS C:\\Users\\aniru\\workspace\\github\\aws-eks\\cluster-autoscaler&gt; kubectl get nodes -l instance-type=spot\nNAME                            STATUS   ROLES    AGE   VERSION\nip-192-168-12-53.ec2.internal   Ready    &lt;none&gt;   15m   v1.23.15-eks-49d8fe8\n</code></pre>"},{"location":"test-cluster-autoscaler/#scale-out-the-deployment","title":"Scale-out the deployment","text":"<p>Let's test scale-out by increasing the number of replicas to 4</p> <pre><code>kubectl scale --replicas=4 deployment/test-autoscaler\n</code></pre> <p>Now check the pods </p> <pre><code>kubectl get pods -o wide\n</code></pre> <pre><code>PS C:\\Users\\aniru\\workspace\\github\\aws-eks\\cluster-autoscaler&gt; kubectl get pods -o wide --watch\nNAME                               READY   STATUS    RESTARTS   AGE     IP       NODE     NOMINATED NODE   READINESS GATES\ntest-autoscaler-6545fc8df4-6qk5f   0/1     Pending   0          4m39s   &lt;none&gt;   &lt;none&gt;   &lt;none&gt;           &lt;none&gt;\ntest-autoscaler-6545fc8df4-7pktn   0/1     Pending   0          10m     &lt;none&gt;   &lt;none&gt;   &lt;none&gt;           &lt;none&gt;\ntest-autoscaler-6545fc8df4-hmrxd   0/1     Pending   0          4m39s   &lt;none&gt;   &lt;none&gt;   &lt;none&gt;           &lt;none&gt;\ntest-autoscaler-6545fc8df4-kfqg4   0/1     Pending   0          13s     &lt;none&gt;   &lt;none&gt;   &lt;none&gt;           &lt;none&gt;\n</code></pre> <p>And let's check the # of instances. It should increase to accomodate the increased # of replicas</p> <pre><code>kubectl get nodes -l instance-type=spot\n</code></pre> <p>Now check page <code>Spot Requests</code> on AWS Console after a few minutes</p>"},{"location":"test-cluster-autoscaler/#scale-in-the-deployment","title":"Scale-in the deployment","text":"<p>Let's test scale-out by reducing the number of replicas to 1</p> <pre><code>kubectl scale --replicas=1 deployment/test-autoscaler\n</code></pre> <p>Now check the pods </p> <pre><code>kubectl get pod\n</code></pre> <pre><code>PS C:\\Users\\aniru\\workspace\\github\\aws-eks\\cluster-autoscaler&gt; kubectl scale --replicas=1 deployment/test-autoscaler \ndeployment.apps/test-autoscaler scaled\nPS C:\\Users\\aniru\\workspace\\github\\aws-eks\\cluster-autoscaler&gt; kubectl get pod\nNAME                               READY   STATUS    RESTARTS   AGE\ntest-autoscaler-6545fc8df4-7pktn   0/1     Pending   0          21m\n</code></pre> <p>And let's check the # of instances. It should increase to accomodate the increased # of replicas</p> <pre><code>kubectl get nodes -l instance-type=spot\n</code></pre> <p>Now check page <code>EC2 &gt; Instances &gt; Spot Requests</code> on AWS Console after a few minutes. Also check page <code>EC2 &gt; Auto Scaling Groups</code></p>"},{"location":"test-cluster-autoscaler/#check-logs","title":"Check logs","text":"<pre><code>kubectl -n kube-system logs deployment.apps/cluster-autoscaler\n</code></pre> <pre><code>S C:\\Users\\aniru\\workspace\\github\\aws-eks\\cluster-autoscaler&gt; kubectl -n kube-system logs deployment.apps/cluster-autoscaler\nFound 2 pods, using pod/cluster-autoscaler-c5bf8cc8-glhlr\n</code></pre>"}]}