{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction This tutorial describes how-to-steps to setup a simple EKS cluster. This tutorial will provide steps for following: How to setup a simple kubernetes cluster using eksctl How to add new nodegroups How to add autoscaler to dynamically scale in and out How to setup a stateless application Application Structure |\u2500\u2500 docs # folder contains all the mkdocs files | \u251c\u2500\u2500 img # Contains all images referenced in mkdocs | \u251c\u2500\u2500 *.md # Other mkdocs .md files \u251c\u2500\u2500 mkdocs.yml # Yaml for for mkdocs \u251c\u2500\u2500 .gitattributes | |\u2500\u2500 cluster-ng # Folder containing a simple EKS cluster with on-demand instances | \u251c\u2500\u2500 eks-cluster.yaml # Yaml file for a simple cluster | \u251c\u2500\u2500 eks-cluster-cmds.md # List of commands to run |\u2500\u2500 cluster-ng-mixed # Folder containing an EKS cluster with spot & on-demand instances | \u251c\u2500\u2500 eks-cluster.yaml # Yaml file for cluster with different node groups |\u2500\u2500 cluster-autoscaler # folder containing a EKS cluster with auto-scaler | \u251c\u2500\u2500 eks-cluster.yaml # Yaml file for a simple cluster | \u251c\u2500\u2500 eks-cluster-cmds.md # List of commands to run |\u2500\u2500 cluster-stateless-app # Folder containing a simple stateless example from kubernetes.io | \u251c\u2500\u2500 frontend-deployment.yaml # Yaml file for frontend deployment | \u251c\u2500\u2500 frontend-service.yaml # Yaml file for frontend service | \u251c\u2500\u2500 README.md # Standard README.md file Credits Although much of the material has been sources from various sources on the web ( including ChatGPT ), I did find this Udemy tutorial very useful in crafting these how-to-steps : Amazon EKS Starter: Docker on AWS EKS with Kubernetes","title":"Introduction"},{"location":"#introduction","text":"This tutorial describes how-to-steps to setup a simple EKS cluster. This tutorial will provide steps for following: How to setup a simple kubernetes cluster using eksctl How to add new nodegroups How to add autoscaler to dynamically scale in and out How to setup a stateless application","title":"Introduction"},{"location":"#application-structure","text":"|\u2500\u2500 docs # folder contains all the mkdocs files | \u251c\u2500\u2500 img # Contains all images referenced in mkdocs | \u251c\u2500\u2500 *.md # Other mkdocs .md files \u251c\u2500\u2500 mkdocs.yml # Yaml for for mkdocs \u251c\u2500\u2500 .gitattributes | |\u2500\u2500 cluster-ng # Folder containing a simple EKS cluster with on-demand instances | \u251c\u2500\u2500 eks-cluster.yaml # Yaml file for a simple cluster | \u251c\u2500\u2500 eks-cluster-cmds.md # List of commands to run |\u2500\u2500 cluster-ng-mixed # Folder containing an EKS cluster with spot & on-demand instances | \u251c\u2500\u2500 eks-cluster.yaml # Yaml file for cluster with different node groups |\u2500\u2500 cluster-autoscaler # folder containing a EKS cluster with auto-scaler | \u251c\u2500\u2500 eks-cluster.yaml # Yaml file for a simple cluster | \u251c\u2500\u2500 eks-cluster-cmds.md # List of commands to run |\u2500\u2500 cluster-stateless-app # Folder containing a simple stateless example from kubernetes.io | \u251c\u2500\u2500 frontend-deployment.yaml # Yaml file for frontend deployment | \u251c\u2500\u2500 frontend-service.yaml # Yaml file for frontend service | \u251c\u2500\u2500 README.md # Standard README.md file","title":"Application Structure"},{"location":"#credits","text":"Although much of the material has been sources from various sources on the web ( including ChatGPT ), I did find this Udemy tutorial very useful in crafting these how-to-steps : Amazon EKS Starter: Docker on AWS EKS with Kubernetes","title":"Credits"},{"location":"clean-up/","text":"Cleaning Up For clean up, first ensure all applications are undeployed using kubectl delete -f <app-deployment-file> kubectl delete -f <app-service-file> First delete all nodegroups eksctl delete nodegroup --cluster=EKS-course-cluster --name=scale-east1c eksctl delete nodegroup --cluster=EKS-course-cluster --name=scale-east1d eksctl delete nodegroup --cluster=EKS-course-cluster --name=scale-spot Now delete the cluster eksctl delete cluster --name=eks-cluster-ng","title":"Clean up"},{"location":"clean-up/#cleaning-up","text":"For clean up, first ensure all applications are undeployed using kubectl delete -f <app-deployment-file> kubectl delete -f <app-service-file> First delete all nodegroups eksctl delete nodegroup --cluster=EKS-course-cluster --name=scale-east1c eksctl delete nodegroup --cluster=EKS-course-cluster --name=scale-east1d eksctl delete nodegroup --cluster=EKS-course-cluster --name=scale-spot Now delete the cluster eksctl delete cluster --name=eks-cluster-ng","title":"Cleaning Up"},{"location":"create-cluster-autoscaler/","text":"Create cluster with autoscaler In this section, we will create a autoscaler that allows us to dynamically scale the nodes within a nodegroup - in and out. We will start by adding 3 new nodegroups to our existing cluster eks-cluster-ng : scale-east1c : Represents on-demand instances of one instance type running on a single AZ for stateful workloads scale-east1d : Represents on-demand instances of multiple instance types running on a single AZ for stateful workloads scale-spot : represents spot instances running on multi AZ for stateless workloads The cluster auto-scaler automatically launches additional worker nodes if more resources are needed, and shutdowns worker nodes if they are under-utilized. The autoscaling works within a nodegroup, hence we will create a nodegroup first which has this feature enabled. Create folder cluster-autoscaler . Copy the following contents into file eks-cluster.yaml : apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: eks-cluster-ng region: us-east-1 nodeGroups: - name: scale-east1c instanceType: t2.nano desiredCapacity: 1 maxSize: 2 availabilityZones: [\"us-east-1c\"] iam: withAddonPolicies: autoScaler: true labels: nodegroup-type: stateful-east1c instance-type: onDemand - name: scale-east1d instanceType: t2.nano desiredCapacity: 1 maxSize: 3 availabilityZones: [\"us-east-1d\"] iam: withAddonPolicies: autoScaler: true labels: nodegroup-type: stateful-east1d instance-type: onDemand ssh: # use existing EC2 key publicKeyName: eks-course - name: scale-spot desiredCapacity: 1 maxSize: 4 instancesDistribution: instanceTypes: [\"t2.micro\", \"t2.small\"] onDemandBaseCapacity: 0 onDemandPercentageAboveBaseCapacity: 0 availabilityZones: [\"us-east-1a\",\"us-east-1c\"] iam: withAddonPolicies: autoScaler: true labels: nodegroup-type: stateless-workload instance-type: spot ssh: publicKeyName: eks-course availabilityZones: [\"us-east-1a\",\"us-east-1c\", \"us-east-1d\"] This will tell EKS to : Apply additional cluster configurations to cluster eks-cluster-ng Apply defintions for 3 nodegroups : scale-east1c : On-demand instances in us-east-1c scale-east1d : On-demand instances in us-east-1d scale-spot : Spot instances in us-east-1a, us-east-1b Note the autoScaler property is set to true Create cluster nodegroups We will now create the new nodegroups in our existing cluster eksctl create nodegroup --config-file=eks-course.yaml kubectl get nodes Let's delete the starter nodegroup ng-1 eksctl delete nodegroup --cluster=eks-cluster-ng --name=ng-1 --approve Create deployment for auto-scaler Deploy the auto-scaler itself: kubectl apply -f https://raw.githubusercontent.com/kubernetes/autoscaler/master/cluster-autoscaler/cloudprovider/aws/examples/cluster-autoscaler-autodiscover.yaml Add annotation to the deployment This prevents from being evicted kubectl -n kube-system annotate deployment.apps/cluster-autoscaler cluster-autoscaler.kubernetes.io/safe-to-evict=\"false\" Set image version and cluster name We will now set matching image version and cluster name eks-cluster-ng in the deployment. Get the autoscaler image version: Open Kubernetes/Autoscalar Releases and get the latest release version matching your Kubernetes version, e.g. Kubernetes 1.14 => check for 1.14.n where \"n\" is the latest release version Edit deployment and set your EKS cluster name: kubectl -n kube-system edit deployment.apps/cluster-autoscaler set the image version at property image=k8s.gcr.io/cluster-autoscaler:vx.yy.z set your EKS cluster name at the end of property - --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled,k8s.io/cluster-autoscaler/<<EKS cluster name>> kubectl -n kube-system describe deployment cluster-autoscaler","title":"Create cluster with autoscaler"},{"location":"create-cluster-autoscaler/#create-cluster-with-autoscaler","text":"In this section, we will create a autoscaler that allows us to dynamically scale the nodes within a nodegroup - in and out. We will start by adding 3 new nodegroups to our existing cluster eks-cluster-ng : scale-east1c : Represents on-demand instances of one instance type running on a single AZ for stateful workloads scale-east1d : Represents on-demand instances of multiple instance types running on a single AZ for stateful workloads scale-spot : represents spot instances running on multi AZ for stateless workloads The cluster auto-scaler automatically launches additional worker nodes if more resources are needed, and shutdowns worker nodes if they are under-utilized. The autoscaling works within a nodegroup, hence we will create a nodegroup first which has this feature enabled. Create folder cluster-autoscaler . Copy the following contents into file eks-cluster.yaml : apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: eks-cluster-ng region: us-east-1 nodeGroups: - name: scale-east1c instanceType: t2.nano desiredCapacity: 1 maxSize: 2 availabilityZones: [\"us-east-1c\"] iam: withAddonPolicies: autoScaler: true labels: nodegroup-type: stateful-east1c instance-type: onDemand - name: scale-east1d instanceType: t2.nano desiredCapacity: 1 maxSize: 3 availabilityZones: [\"us-east-1d\"] iam: withAddonPolicies: autoScaler: true labels: nodegroup-type: stateful-east1d instance-type: onDemand ssh: # use existing EC2 key publicKeyName: eks-course - name: scale-spot desiredCapacity: 1 maxSize: 4 instancesDistribution: instanceTypes: [\"t2.micro\", \"t2.small\"] onDemandBaseCapacity: 0 onDemandPercentageAboveBaseCapacity: 0 availabilityZones: [\"us-east-1a\",\"us-east-1c\"] iam: withAddonPolicies: autoScaler: true labels: nodegroup-type: stateless-workload instance-type: spot ssh: publicKeyName: eks-course availabilityZones: [\"us-east-1a\",\"us-east-1c\", \"us-east-1d\"] This will tell EKS to : Apply additional cluster configurations to cluster eks-cluster-ng Apply defintions for 3 nodegroups : scale-east1c : On-demand instances in us-east-1c scale-east1d : On-demand instances in us-east-1d scale-spot : Spot instances in us-east-1a, us-east-1b Note the autoScaler property is set to true","title":"Create cluster with autoscaler"},{"location":"create-cluster-autoscaler/#create-cluster-nodegroups","text":"We will now create the new nodegroups in our existing cluster eksctl create nodegroup --config-file=eks-course.yaml kubectl get nodes Let's delete the starter nodegroup ng-1 eksctl delete nodegroup --cluster=eks-cluster-ng --name=ng-1 --approve","title":"Create cluster nodegroups"},{"location":"create-cluster-autoscaler/#create-deployment-for-auto-scaler","text":"Deploy the auto-scaler itself: kubectl apply -f https://raw.githubusercontent.com/kubernetes/autoscaler/master/cluster-autoscaler/cloudprovider/aws/examples/cluster-autoscaler-autodiscover.yaml","title":"Create deployment for auto-scaler"},{"location":"create-cluster-autoscaler/#add-annotation-to-the-deployment","text":"This prevents from being evicted kubectl -n kube-system annotate deployment.apps/cluster-autoscaler cluster-autoscaler.kubernetes.io/safe-to-evict=\"false\"","title":"Add annotation to the deployment"},{"location":"create-cluster-autoscaler/#set-image-version-and-cluster-name","text":"We will now set matching image version and cluster name eks-cluster-ng in the deployment. Get the autoscaler image version: Open Kubernetes/Autoscalar Releases and get the latest release version matching your Kubernetes version, e.g. Kubernetes 1.14 => check for 1.14.n where \"n\" is the latest release version Edit deployment and set your EKS cluster name: kubectl -n kube-system edit deployment.apps/cluster-autoscaler set the image version at property image=k8s.gcr.io/cluster-autoscaler:vx.yy.z set your EKS cluster name at the end of property - --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled,k8s.io/cluster-autoscaler/<<EKS cluster name>> kubectl -n kube-system describe deployment cluster-autoscaler","title":"Set image version and cluster name"},{"location":"create-cluster/","text":"Create simple Cluster In this section, we will create a simple cluster using eksctl Create Yaml file Create folder cluster-ng . The first step is to create a simple yaml file in this folder. Let's call it eks-cluster.yaml . Copy the following contents to it: apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: eks-cluster-ng region: us-east-1 nodeGroups: - name: ng-1 instanceType: t2.nano desiredCapacity: 2 ssh: # use existing EC2 key publicKeyName: eks-course This tells EKS to: Create a cluster eks-cluster-ng in region us-east-1 Call the name of the nodegroup : ng-1 Set the instance type as :t2.micro Set the desired capcity of instances as 2 Set ssh key : eks-course Create cluster Now we will create the cluster by specfying the file to use with flag f with command : eksctl create cluster eksctl create cluster -f eks-cluster.yaml This will take 10-15 minutes. Now log into AWS console and navigate to the Cloud Formation service. You will see that eksctl creates a cloud formation stack. Check the resources tab to glace through various resources being created. Check cluster Now let's check the EC2 console. You will see 2 instances : Let's use the eksctl get nodegroup command to get the details of the nodegroup we just created : eksctl get nodegroup --cluster eks-cluster-ng S C:\\Users\\aniru\\workspace\\github\\aws-eks> eksctl get nodegroup --cluster eks-cluster-ng CLUSTER NODEGROUP STATUS CREATED MIN SIZE MAX SIZE DESIRED CAPACITY INSTANCE TYPE IMAGE ID ASG NAME TYPE eks-cluster-ng ng-1 CREATE_COMPLETE 2023-01-25T00:22:41Z 2 2 2 t2.nano ami-03a30cc1dda93f173 eksctl-eks-cluster-ng-nodegroup-ng-1-NodeGroup-6387ICJKD3BS unmanaged Let's use the kubectl command to get the nodes : kubectl get nodes PS C:\\Users\\aniru\\workspace\\github\\aws-eks> kubectl get nodes Kubeconfig user entry is using deprecated API version client.authentication.k8s.io/v1alpha1. Run 'aws eks update-kubeconfig' to update. NAME STATUS ROLES AGE VERSION ip-192-168-21-105.ec2.internal Ready <none> 71m v1.22.15-eks-fb459a0 ip-192-168-54-187.ec2.internal Ready <none> 71m v1.22.15-eks-fb459a0","title":"Create cluster"},{"location":"create-cluster/#create-simple-cluster","text":"In this section, we will create a simple cluster using eksctl","title":"Create simple Cluster"},{"location":"create-cluster/#create-yaml-file","text":"Create folder cluster-ng . The first step is to create a simple yaml file in this folder. Let's call it eks-cluster.yaml . Copy the following contents to it: apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: eks-cluster-ng region: us-east-1 nodeGroups: - name: ng-1 instanceType: t2.nano desiredCapacity: 2 ssh: # use existing EC2 key publicKeyName: eks-course This tells EKS to: Create a cluster eks-cluster-ng in region us-east-1 Call the name of the nodegroup : ng-1 Set the instance type as :t2.micro Set the desired capcity of instances as 2 Set ssh key : eks-course","title":"Create Yaml file"},{"location":"create-cluster/#create-cluster","text":"Now we will create the cluster by specfying the file to use with flag f with command : eksctl create cluster eksctl create cluster -f eks-cluster.yaml This will take 10-15 minutes. Now log into AWS console and navigate to the Cloud Formation service. You will see that eksctl creates a cloud formation stack. Check the resources tab to glace through various resources being created.","title":"Create cluster"},{"location":"create-cluster/#check-cluster","text":"Now let's check the EC2 console. You will see 2 instances : Let's use the eksctl get nodegroup command to get the details of the nodegroup we just created : eksctl get nodegroup --cluster eks-cluster-ng S C:\\Users\\aniru\\workspace\\github\\aws-eks> eksctl get nodegroup --cluster eks-cluster-ng CLUSTER NODEGROUP STATUS CREATED MIN SIZE MAX SIZE DESIRED CAPACITY INSTANCE TYPE IMAGE ID ASG NAME TYPE eks-cluster-ng ng-1 CREATE_COMPLETE 2023-01-25T00:22:41Z 2 2 2 t2.nano ami-03a30cc1dda93f173 eksctl-eks-cluster-ng-nodegroup-ng-1-NodeGroup-6387ICJKD3BS unmanaged Let's use the kubectl command to get the nodes : kubectl get nodes PS C:\\Users\\aniru\\workspace\\github\\aws-eks> kubectl get nodes Kubeconfig user entry is using deprecated API version client.authentication.k8s.io/v1alpha1. Run 'aws eks update-kubeconfig' to update. NAME STATUS ROLES AGE VERSION ip-192-168-21-105.ec2.internal Ready <none> 71m v1.22.15-eks-fb459a0 ip-192-168-54-187.ec2.internal Ready <none> 71m v1.22.15-eks-fb459a0","title":"Check cluster"},{"location":"create-stateless-app/","text":"Create Stateless App In this section we will : Deploy backend resources Deploy frontend resources Scale pods up/down Perform chaos testing Setup sample guestbook app This example is based on Kubernetes Guestbook . The application consists of: A Redis backend with a single leader pod for Writes & multiple follower pods for Reads A frontend app ( Guestbook app ) in PHP loadbalanced to public using a load balancer Guestbook app reads load balanced to multiple redis followers (reads) Guestbook app writes directed towards single redis leader (writes) Backend Deployment Our configuration files for Redis leader/followers includes both Deployment and Services kinds. Deployments and Services are often used in tandem: Deployments working to define the desired state of the application and Services working to make sure communication between almost any kind of resource and the rest of the cluster is stable and adaptable. Redis leader For the Redis leader, first create the Deployment file redis-leader-deployment.yaml . Copy these contents to it : # SOURCE: https://cloud.google.com/kubernetes-engine/docs/tutorials/guestbook apiVersion: apps/v1 kind: Deployment metadata: name: redis-leader labels: app: redis role: leader tier: backend spec: replicas: 1 selector: matchLabels: app: redis template: metadata: labels: app: redis role: leader tier: backend spec: containers: - name: leader image: \"docker.io/redis:6.0.5\" resources: requests: cpu: 100m memory: 100Mi ports: - containerPort: 6379 This tells EKS : The deployment has replica of 1 Specifies the source of image Set resource requirement for cpu & memory Set the container port to 6379 Now let's create the Service on top of your deployment. Create file redis-leader-service.yaml and copy the following contents to it: # SOURCE: https://cloud.google.com/kubernetes-engine/docs/tutorials/guestbook apiVersion: v1 kind: Service metadata: name: redis-leader labels: app: redis role: leader tier: backend spec: ports: - port: 6379 targetPort: 6379 selector: app: redis role: leader tier: backend Notice that the metadata name and labels are same as the redis-leader Deployment. Here the target port that is exposed is set to 6379. Now that the configuration files are defined, let's deploy the Redis leader pod : kubectl apply -f redis-leader-deployment.yaml Let's test by checking if the pods were created kubectl get pods Now we apply Redis service on top of this : kubectl apply -f redis-leader-service.yaml Let's test using : kubectl get service redis-leader Redis follower For the Redis follower, let's create the Deployment file redis-followers-deployment.yaml . Copy these contents to it : # SOURCE: https://cloud.google.com/kubernetes-engine/docs/tutorials/guestbook apiVersion: apps/v1 kind: Deployment metadata: name: redis-leader labels: app: redis role: leader tier: backend spec: replicas: 1 selector: matchLabels: app: redis template: metadata: labels: app: redis role: leader tier: backend spec: containers: - name: leader image: \"docker.io/redis:6.0.5\" resources: requests: cpu: 100m memory: 100Mi ports: - containerPort: 6379 Just like before we will now create the Service file as redis-follower-service.yaml . Copy the following contents to it : # SOURCE: https://cloud.google.com/kubernetes-engine/docs/tutorials/guestbook apiVersion: v1 kind: Service metadata: name: redis-follower labels: app: redis role: follower tier: backend spec: ports: # the port that this service should serve on - port: 6379 selector: app: redis role: follower tier: backend Notice the labels that match the Deployment file and the internal port that the redis follower service should run on. Now that the configuration files are defined, let's deploy the Redis follower pod : kubectl apply -f redis-followers-deployment.yaml kubectl apply -f redis-follower-service.yaml Let's check the status of Redis followers by running : kubectl get pods -o wide kubectl get service kubectl describe node <node-name> Frontend app Now we will deploy the frontend app that will consist of 3 replicas of the Guestbook App ELB LoadBalancer service Guestbook deployment We will start with defining the Deployment file for the frontend app. Create frontend-deployment.yaml and copy the following contents : # SOURCE: https://cloud.google.com/kubernetes-engine/docs/tutorials/guestbook apiVersion: apps/v1 kind: Deployment metadata: name: frontend spec: replicas: 3 selector: matchLabels: app: guestbook tier: frontend template: metadata: labels: app: guestbook tier: frontend spec: containers: - name: php-redis image: gcr.io/google_samples/gb-frontend:v5 env: - name: GET_HOSTS_FROM value: \"dns\" resources: requests: cpu: 100m memory: 100Mi ports: - containerPort: 80 This tells EKS to : Generate 3 replicas of the frontend app Specfiy image source Set the container resources Set the container port Let's apply the deployment kubectl apply -f frontend-deployment.yaml Check all the pods for the guestbook app kubectl get pods kubectl get pods -l app=guestbook kubectl get pods -l app=guestbook -l tier=frontend Loadbalancer service We will now apply the loadbalancer service that allows us to expose all the loadbalanced service to the public. Create file frontend-service.yaml and copy the following : # SOURCE: https://cloud.google.com/kubernetes-engine/docs/tutorials/guestbook apiVersion: v1 kind: Service metadata: name: frontend labels: app: guestbook tier: frontend spec: # if your cluster supports it, uncomment the following to automatically create # an external load-balanced IP for the frontend service. # type: LoadBalancer type: LoadBalancer ports: # the port that this service should serve on - port: 80 selector: app: guestbook tier: frontend Let's apply the loadbalanced service kubectl apply -f frontend-service.yaml Check all services kubectl get services kubectl get service frontend kubetctl get pods -o wide kubectl describe service frontend Finally check AWS mgm console for the ELB which has been created ! Access from outside the cluster Grab the public DNS of the frontend service LoadBalancer (ELB): kubectl describe service frontend Copy the name and paste it into your browser !!! Scale Pods up and down We can scale pods up and down in the following ways: kubectl Update YAML file Using kubectl We can scale up and down using kubectl command. To scale up run the following command : kubectl scale deployment frontend --replicas=5 Let's check the pods now kubectl get pods Now let us scale it down kubectl scale deployment frontend --replicas=3 You can check pods are terminating by checking the pods kubectl get pods Using YAML file We can scale the pods by updating the replicas tag in our yaml files. Let's test this out by updating file redis-followers-deployment.yaml . Update the replicas count to 5 as follows : # SOURCE: https://cloud.google.com/kubernetes-engine/docs/tutorials/guestbook apiVersion: apps/v1 kind: Deployment metadata: name: frontend spec: replicas: 5 selector: matchLabels: app: guestbook tier: frontend template: metadata: labels: app: guestbook tier: frontend spec: containers: - name: php-redis image: gcr.io/google_samples/gb-frontend:v5 env: - name: GET_HOSTS_FROM value: \"dns\" resources: requests: cpu: 100m memory: 100Mi ports: - containerPort: 80 Let's apply the new deployment file kubectl apply -f frontend-deployment.yaml Let's check this by running kubectl get deployment frontend kubectl get pods Perform chaos testing We will demonstrate self-healing mechanism of K8s by: Kill pods Stop K8s worker node(s) kubectl get pods kubectl get nodes kubectl get pods -o wide Now let's delete one of front-end pods kubectl delete pod <frontend-pod> Now check for pods look a the age column. Kubernetes restarts the pod kubectl get pods -o wide Now let's use AWS console to stop any instance ( AWS > EC2 > instances). You will notice 2 things : * Kubernetes will try to recover the nodes * Kubernetes will reshuffle the nodes if required Cleaning up Run the following commands to save costs kubectl delete deployment -l app=redis kubectl delete service -l app=redis kubectl delete deployment frontend kubectl delete service frontend","title":"Create stateless app"},{"location":"create-stateless-app/#create-stateless-app","text":"In this section we will : Deploy backend resources Deploy frontend resources Scale pods up/down Perform chaos testing","title":"Create Stateless App"},{"location":"create-stateless-app/#setup-sample-guestbook-app","text":"This example is based on Kubernetes Guestbook . The application consists of: A Redis backend with a single leader pod for Writes & multiple follower pods for Reads A frontend app ( Guestbook app ) in PHP loadbalanced to public using a load balancer Guestbook app reads load balanced to multiple redis followers (reads) Guestbook app writes directed towards single redis leader (writes)","title":"Setup sample guestbook app"},{"location":"create-stateless-app/#backend-deployment","text":"Our configuration files for Redis leader/followers includes both Deployment and Services kinds. Deployments and Services are often used in tandem: Deployments working to define the desired state of the application and Services working to make sure communication between almost any kind of resource and the rest of the cluster is stable and adaptable.","title":"Backend Deployment"},{"location":"create-stateless-app/#redis-leader","text":"For the Redis leader, first create the Deployment file redis-leader-deployment.yaml . Copy these contents to it : # SOURCE: https://cloud.google.com/kubernetes-engine/docs/tutorials/guestbook apiVersion: apps/v1 kind: Deployment metadata: name: redis-leader labels: app: redis role: leader tier: backend spec: replicas: 1 selector: matchLabels: app: redis template: metadata: labels: app: redis role: leader tier: backend spec: containers: - name: leader image: \"docker.io/redis:6.0.5\" resources: requests: cpu: 100m memory: 100Mi ports: - containerPort: 6379 This tells EKS : The deployment has replica of 1 Specifies the source of image Set resource requirement for cpu & memory Set the container port to 6379 Now let's create the Service on top of your deployment. Create file redis-leader-service.yaml and copy the following contents to it: # SOURCE: https://cloud.google.com/kubernetes-engine/docs/tutorials/guestbook apiVersion: v1 kind: Service metadata: name: redis-leader labels: app: redis role: leader tier: backend spec: ports: - port: 6379 targetPort: 6379 selector: app: redis role: leader tier: backend Notice that the metadata name and labels are same as the redis-leader Deployment. Here the target port that is exposed is set to 6379. Now that the configuration files are defined, let's deploy the Redis leader pod : kubectl apply -f redis-leader-deployment.yaml Let's test by checking if the pods were created kubectl get pods Now we apply Redis service on top of this : kubectl apply -f redis-leader-service.yaml Let's test using : kubectl get service redis-leader","title":"Redis leader"},{"location":"create-stateless-app/#redis-follower","text":"For the Redis follower, let's create the Deployment file redis-followers-deployment.yaml . Copy these contents to it : # SOURCE: https://cloud.google.com/kubernetes-engine/docs/tutorials/guestbook apiVersion: apps/v1 kind: Deployment metadata: name: redis-leader labels: app: redis role: leader tier: backend spec: replicas: 1 selector: matchLabels: app: redis template: metadata: labels: app: redis role: leader tier: backend spec: containers: - name: leader image: \"docker.io/redis:6.0.5\" resources: requests: cpu: 100m memory: 100Mi ports: - containerPort: 6379 Just like before we will now create the Service file as redis-follower-service.yaml . Copy the following contents to it : # SOURCE: https://cloud.google.com/kubernetes-engine/docs/tutorials/guestbook apiVersion: v1 kind: Service metadata: name: redis-follower labels: app: redis role: follower tier: backend spec: ports: # the port that this service should serve on - port: 6379 selector: app: redis role: follower tier: backend Notice the labels that match the Deployment file and the internal port that the redis follower service should run on. Now that the configuration files are defined, let's deploy the Redis follower pod : kubectl apply -f redis-followers-deployment.yaml kubectl apply -f redis-follower-service.yaml Let's check the status of Redis followers by running : kubectl get pods -o wide kubectl get service kubectl describe node <node-name>","title":"Redis follower"},{"location":"create-stateless-app/#frontend-app","text":"Now we will deploy the frontend app that will consist of 3 replicas of the Guestbook App ELB LoadBalancer service","title":"Frontend app"},{"location":"create-stateless-app/#guestbook-deployment","text":"We will start with defining the Deployment file for the frontend app. Create frontend-deployment.yaml and copy the following contents : # SOURCE: https://cloud.google.com/kubernetes-engine/docs/tutorials/guestbook apiVersion: apps/v1 kind: Deployment metadata: name: frontend spec: replicas: 3 selector: matchLabels: app: guestbook tier: frontend template: metadata: labels: app: guestbook tier: frontend spec: containers: - name: php-redis image: gcr.io/google_samples/gb-frontend:v5 env: - name: GET_HOSTS_FROM value: \"dns\" resources: requests: cpu: 100m memory: 100Mi ports: - containerPort: 80 This tells EKS to : Generate 3 replicas of the frontend app Specfiy image source Set the container resources Set the container port Let's apply the deployment kubectl apply -f frontend-deployment.yaml Check all the pods for the guestbook app kubectl get pods kubectl get pods -l app=guestbook kubectl get pods -l app=guestbook -l tier=frontend","title":"Guestbook deployment"},{"location":"create-stateless-app/#loadbalancer-service","text":"We will now apply the loadbalancer service that allows us to expose all the loadbalanced service to the public. Create file frontend-service.yaml and copy the following : # SOURCE: https://cloud.google.com/kubernetes-engine/docs/tutorials/guestbook apiVersion: v1 kind: Service metadata: name: frontend labels: app: guestbook tier: frontend spec: # if your cluster supports it, uncomment the following to automatically create # an external load-balanced IP for the frontend service. # type: LoadBalancer type: LoadBalancer ports: # the port that this service should serve on - port: 80 selector: app: guestbook tier: frontend Let's apply the loadbalanced service kubectl apply -f frontend-service.yaml Check all services kubectl get services kubectl get service frontend kubetctl get pods -o wide kubectl describe service frontend Finally check AWS mgm console for the ELB which has been created !","title":"Loadbalancer service"},{"location":"create-stateless-app/#access-from-outside-the-cluster","text":"Grab the public DNS of the frontend service LoadBalancer (ELB): kubectl describe service frontend Copy the name and paste it into your browser !!!","title":"Access from outside the cluster"},{"location":"create-stateless-app/#scale-pods-up-and-down","text":"We can scale pods up and down in the following ways: kubectl Update YAML file","title":"Scale Pods up and down"},{"location":"create-stateless-app/#using-kubectl","text":"We can scale up and down using kubectl command. To scale up run the following command : kubectl scale deployment frontend --replicas=5 Let's check the pods now kubectl get pods Now let us scale it down kubectl scale deployment frontend --replicas=3 You can check pods are terminating by checking the pods kubectl get pods","title":"Using kubectl"},{"location":"create-stateless-app/#using-yaml-file","text":"We can scale the pods by updating the replicas tag in our yaml files. Let's test this out by updating file redis-followers-deployment.yaml . Update the replicas count to 5 as follows : # SOURCE: https://cloud.google.com/kubernetes-engine/docs/tutorials/guestbook apiVersion: apps/v1 kind: Deployment metadata: name: frontend spec: replicas: 5 selector: matchLabels: app: guestbook tier: frontend template: metadata: labels: app: guestbook tier: frontend spec: containers: - name: php-redis image: gcr.io/google_samples/gb-frontend:v5 env: - name: GET_HOSTS_FROM value: \"dns\" resources: requests: cpu: 100m memory: 100Mi ports: - containerPort: 80 Let's apply the new deployment file kubectl apply -f frontend-deployment.yaml Let's check this by running kubectl get deployment frontend kubectl get pods","title":"Using YAML file"},{"location":"create-stateless-app/#perform-chaos-testing","text":"We will demonstrate self-healing mechanism of K8s by: Kill pods Stop K8s worker node(s) kubectl get pods kubectl get nodes kubectl get pods -o wide Now let's delete one of front-end pods kubectl delete pod <frontend-pod> Now check for pods look a the age column. Kubernetes restarts the pod kubectl get pods -o wide Now let's use AWS console to stop any instance ( AWS > EC2 > instances). You will notice 2 things : * Kubernetes will try to recover the nodes * Kubernetes will reshuffle the nodes if required","title":"Perform chaos testing"},{"location":"create-stateless-app/#cleaning-up","text":"Run the following commands to save costs kubectl delete deployment -l app=redis kubectl delete service -l app=redis kubectl delete deployment frontend kubectl delete service frontend","title":"Cleaning up"},{"location":"extend-cluster/","text":"Extend Cluster In this section we will extend the cluster by adding a nodegroup that contains a mix of: On-demand instances Spot instances Define Yaml file Create folder cluster-ng-mixed . We will now extend the cluster by adding a nodegroup to file eks-cluster.yaml . The file adds a new nodegroup ng-mixed . Copy the following contents : apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: eks-cluster-ng region: us-east-1 nodeGroups: - name: ng-1 instanceType: t2.nano desiredCapacity: 2 ssh: # use existing EC2 key publicKeyName: eks-course - name: ng-mixed minSize: 3 maxSize: 5 instancesDistribution: maxPrice: 0.2 instanceTypes: [\"t2.small\", \"t3.small\"] onDemandBaseCapacity: 0 onDemandPercentageAboveBaseCapacity: 50 ssh: publicKeyName: eks-course This tells EKS to : Add a new nodegroup ng-mixed Set min & max size Define the instance types 50% of the instance types are on-demand and the rest on spot Add nodegroup First check your cluster is running : eksctl get cluster Let's add the new nodegroup : eksctl create nodegroup --config-file=eks-cluster.yaml --include='ng-mixed' Delete nodegroup Now let's drain the ng-mixed nodegroup we just created so we don't incur additional costs : eksctl delete nodegroup --config-file=eks-cluster.yaml --include='ng-mixed' --approve","title":"Extend cluster"},{"location":"extend-cluster/#extend-cluster","text":"In this section we will extend the cluster by adding a nodegroup that contains a mix of: On-demand instances Spot instances","title":"Extend Cluster"},{"location":"extend-cluster/#define-yaml-file","text":"Create folder cluster-ng-mixed . We will now extend the cluster by adding a nodegroup to file eks-cluster.yaml . The file adds a new nodegroup ng-mixed . Copy the following contents : apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: eks-cluster-ng region: us-east-1 nodeGroups: - name: ng-1 instanceType: t2.nano desiredCapacity: 2 ssh: # use existing EC2 key publicKeyName: eks-course - name: ng-mixed minSize: 3 maxSize: 5 instancesDistribution: maxPrice: 0.2 instanceTypes: [\"t2.small\", \"t3.small\"] onDemandBaseCapacity: 0 onDemandPercentageAboveBaseCapacity: 50 ssh: publicKeyName: eks-course This tells EKS to : Add a new nodegroup ng-mixed Set min & max size Define the instance types 50% of the instance types are on-demand and the rest on spot","title":"Define Yaml file"},{"location":"extend-cluster/#add-nodegroup","text":"First check your cluster is running : eksctl get cluster Let's add the new nodegroup : eksctl create nodegroup --config-file=eks-cluster.yaml --include='ng-mixed'","title":"Add nodegroup"},{"location":"extend-cluster/#delete-nodegroup","text":"Now let's drain the ng-mixed nodegroup we just created so we don't incur additional costs : eksctl delete nodegroup --config-file=eks-cluster.yaml --include='ng-mixed' --approve","title":"Delete nodegroup"},{"location":"local-environment/","text":"Local Enviornment Configurations Navigate to your AWS credentials file and update the default profile with access-keys of the newly created IAM user PS C:\\Users\\aniru\\.aws> cd ~\\.aws PS C:\\Users\\aniru\\.aws> ls Directory: C:\\Users\\aniru\\.aws Mode LastWriteTime Length Name ---- ------------- ------ ---- -a---- 11/1/2022 10:50 AM 99 config -a---- 1/24/2023 5:19 PM 461 credentials Edit credentials file by updating the [default] entry as follows: [default] aws_access_key_id=### aws_secret_access_key=### region=us-east-1 output=json","title":"Local environment"},{"location":"local-environment/#local-enviornment","text":"","title":"Local Enviornment"},{"location":"local-environment/#configurations","text":"Navigate to your AWS credentials file and update the default profile with access-keys of the newly created IAM user PS C:\\Users\\aniru\\.aws> cd ~\\.aws PS C:\\Users\\aniru\\.aws> ls Directory: C:\\Users\\aniru\\.aws Mode LastWriteTime Length Name ---- ------------- ------ ---- -a---- 11/1/2022 10:50 AM 99 config -a---- 1/24/2023 5:19 PM 461 credentials Edit credentials file by updating the [default] entry as follows: [default] aws_access_key_id=### aws_secret_access_key=### region=us-east-1 output=json","title":"Configurations"},{"location":"local-installations/","text":"Local Installations Install AWS CLI Follow the instructions from this page: Installing or updating the latest version of the AWS CLI Test PS C:\\Users\\aniru\\.aws> aws --version aws-cli/2.7.4 Python/3.9.11 Windows/10 exe/AMD64 prompt/off Install eksctl Follow the installation instructions from here : eksctl installation Test PS C:\\Users\\aniru\\.aws> eksctl version 0.99.0 Install kubectl Follow instructions from here: kubectl installation Test PS C:\\Users\\aniru\\.aws> kubectl version --client Client Version: version.Info{Major:\"1\", Minor:\"23\", GitVersion:\"v1.23.6\", GitCommit:\"ad3338546da947756e8a88aa6822e9c11e7eac22\", GitTreeState:\"clean\", BuildDate:\"2022-04-14T08:49:13Z\", GoVersion:\"go1.17.9\", Compiler:\"gc\", Platform:\"windows/amd64\"}","title":"Local installations"},{"location":"local-installations/#local-installations","text":"","title":"Local Installations"},{"location":"local-installations/#install-aws-cli","text":"Follow the instructions from this page: Installing or updating the latest version of the AWS CLI Test PS C:\\Users\\aniru\\.aws> aws --version aws-cli/2.7.4 Python/3.9.11 Windows/10 exe/AMD64 prompt/off","title":"Install AWS CLI"},{"location":"local-installations/#install-eksctl","text":"Follow the installation instructions from here : eksctl installation Test PS C:\\Users\\aniru\\.aws> eksctl version 0.99.0","title":"Install eksctl"},{"location":"local-installations/#install-kubectl","text":"Follow instructions from here: kubectl installation Test PS C:\\Users\\aniru\\.aws> kubectl version --client Client Version: version.Info{Major:\"1\", Minor:\"23\", GitVersion:\"v1.23.6\", GitCommit:\"ad3338546da947756e8a88aa6822e9c11e7eac22\", GitTreeState:\"clean\", BuildDate:\"2022-04-14T08:49:13Z\", GoVersion:\"go1.17.9\", Compiler:\"gc\", Platform:\"windows/amd64\"}","title":"Install kubectl"},{"location":"prerequisites/","text":"Prerequisites IAM user and permissions We will create a new IAM user ( eks-user ) for the purposes of the tutorial. Please keep in mind that you should never use your root account for working with AWS services. Create policies Create these 2 policies: EKS-Admin-policy: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"eks:*\" ], \"Resource\": \"*\" } ] } CloudFormation-Admin-policy: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"cloudformation:*\" ], \"Resource\": \"*\" } ] } Assign policies Finally, assign the following policies to your IAM user you are going to use throughout the course: AmazonEC2FullAccess IAMFullAccess AmazonVPCFullAccess CloudFormation-Admin-policy EKS-Admin-policy where the last 2 policies are the ones you created above Create IAM role Open https://console.aws.amazon.com/iam/ and choose Roles => create role Choose EKS service followed by Allows Amazon EKS to manage your clusters on your behalf Choose Next: Permissions Click Next: Review Enter a unique Role name, EKS-course-role and click Create Role Create key Pair Open EC2 dashboard https://console.aws.amazon.com/ec2 Click KeyPairs in left navigation bar under section \"Network&Security\" Click Create Key Pair Provide name for keypair, eks-course and click Create The keypair will be downloaded immediately => file eks-course.pem Create API Access key/secret Create key+secret via AWS console AWS-console => IAM => Users => => tab Security credentials => button Create access key","title":"Prerequisites"},{"location":"prerequisites/#prerequisites","text":"","title":"Prerequisites"},{"location":"prerequisites/#iam-user-and-permissions","text":"We will create a new IAM user ( eks-user ) for the purposes of the tutorial. Please keep in mind that you should never use your root account for working with AWS services.","title":"IAM user and permissions"},{"location":"prerequisites/#create-policies","text":"Create these 2 policies: EKS-Admin-policy: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"eks:*\" ], \"Resource\": \"*\" } ] } CloudFormation-Admin-policy: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"cloudformation:*\" ], \"Resource\": \"*\" } ] }","title":"Create policies"},{"location":"prerequisites/#assign-policies","text":"Finally, assign the following policies to your IAM user you are going to use throughout the course: AmazonEC2FullAccess IAMFullAccess AmazonVPCFullAccess CloudFormation-Admin-policy EKS-Admin-policy where the last 2 policies are the ones you created above","title":"Assign policies"},{"location":"prerequisites/#create-iam-role","text":"Open https://console.aws.amazon.com/iam/ and choose Roles => create role Choose EKS service followed by Allows Amazon EKS to manage your clusters on your behalf Choose Next: Permissions Click Next: Review Enter a unique Role name, EKS-course-role and click Create Role","title":"Create IAM role"},{"location":"prerequisites/#create-key-pair","text":"Open EC2 dashboard https://console.aws.amazon.com/ec2 Click KeyPairs in left navigation bar under section \"Network&Security\" Click Create Key Pair Provide name for keypair, eks-course and click Create The keypair will be downloaded immediately => file eks-course.pem","title":"Create key Pair"},{"location":"prerequisites/#create-api-access-keysecret","text":"Create key+secret via AWS console AWS-console => IAM => Users => => tab Security credentials => button Create access key","title":"Create API Access key/secret"},{"location":"test-cluster-autoscaler/","text":"Test Auto-scaler with Nginx deployment We will now test the auto-scaler and scale it up and down Create the deployment file In folder cluster-autoscaler , we will create a new deployment file nginx-deployment.yaml . Copy the following contents to it: apiVersion: apps/v1 kind: Deployment metadata: name: test-autoscaler spec: replicas: 1 selector: matchLabels: app: nginx template: metadata: labels: service: nginx app: nginx spec: containers: - image: nginx name: test-autoscaler resources: limits: cpu: 300m memory: 512Mi requests: cpu: 300m memory: 512Mi nodeSelector: instance-type: spot This will tell eksctl to : Refer to this one as of kind: Deployment This will have 1 replica with service: nginx and app: nginx Container will use image: nginx and will call it as name: test-autoscaler Specify resources with cpu & memory tags nodeSelector specifies what kind of instances we will need using tag instance-type: spot Deploy Nginx We will now apply the deployment file using kubectl command kubectl apply -f nginx-deployment.yaml Now check if the pod is running kubectl get pod check the instance types kubectl get nodes -l instance-type=spot Scale the deployment Let's scale deployment to 3 replicas so we can test the autoscaler kubectl scale --replicas=3 deployment/test-autoscaler You should see 3 pods running now but using the same node kubectl get pod Scale-out the deployment Let's test scale-out by increasing the number of replicas to 4 kubectl scale --replicas=4 deployment/test-autoscaler Now check the pods kubectl get pods -o wide --watch And let's check the # of instances. It should increase to accomodate the increased # of replicas kubectl get nodes -l instance-type=spot Now check page Spot Requests on AWS Console after a few minutes Scale-in the deployment Let's test scale-out by reducing the number of replicas to 1 kubectl scale --replicas=1 deployment/test-autoscaler Now check the pods kubectl get pods -o wide --watch And let's check the # of instances. It should increase to accomodate the increased # of replicas kubectl get nodes -l instance-type=spot Now check page EC2 > Instances > Spot Requests on AWS Console after a few minutes. Also check page EC2 > Auto Scaling Groups Check logs kubectl -n kube-system logs deployment.apps/cluster-autoscaler","title":"Test autoscaler with Nginx"},{"location":"test-cluster-autoscaler/#test-auto-scaler-with-nginx-deployment","text":"We will now test the auto-scaler and scale it up and down","title":"Test Auto-scaler with Nginx deployment"},{"location":"test-cluster-autoscaler/#create-the-deployment-file","text":"In folder cluster-autoscaler , we will create a new deployment file nginx-deployment.yaml . Copy the following contents to it: apiVersion: apps/v1 kind: Deployment metadata: name: test-autoscaler spec: replicas: 1 selector: matchLabels: app: nginx template: metadata: labels: service: nginx app: nginx spec: containers: - image: nginx name: test-autoscaler resources: limits: cpu: 300m memory: 512Mi requests: cpu: 300m memory: 512Mi nodeSelector: instance-type: spot This will tell eksctl to : Refer to this one as of kind: Deployment This will have 1 replica with service: nginx and app: nginx Container will use image: nginx and will call it as name: test-autoscaler Specify resources with cpu & memory tags nodeSelector specifies what kind of instances we will need using tag instance-type: spot","title":"Create the deployment file"},{"location":"test-cluster-autoscaler/#deploy-nginx","text":"We will now apply the deployment file using kubectl command kubectl apply -f nginx-deployment.yaml Now check if the pod is running kubectl get pod check the instance types kubectl get nodes -l instance-type=spot","title":"Deploy Nginx"},{"location":"test-cluster-autoscaler/#scale-the-deployment","text":"Let's scale deployment to 3 replicas so we can test the autoscaler kubectl scale --replicas=3 deployment/test-autoscaler You should see 3 pods running now but using the same node kubectl get pod","title":"Scale the deployment"},{"location":"test-cluster-autoscaler/#scale-out-the-deployment","text":"Let's test scale-out by increasing the number of replicas to 4 kubectl scale --replicas=4 deployment/test-autoscaler Now check the pods kubectl get pods -o wide --watch And let's check the # of instances. It should increase to accomodate the increased # of replicas kubectl get nodes -l instance-type=spot Now check page Spot Requests on AWS Console after a few minutes","title":"Scale-out the deployment"},{"location":"test-cluster-autoscaler/#scale-in-the-deployment","text":"Let's test scale-out by reducing the number of replicas to 1 kubectl scale --replicas=1 deployment/test-autoscaler Now check the pods kubectl get pods -o wide --watch And let's check the # of instances. It should increase to accomodate the increased # of replicas kubectl get nodes -l instance-type=spot Now check page EC2 > Instances > Spot Requests on AWS Console after a few minutes. Also check page EC2 > Auto Scaling Groups","title":"Scale-in the deployment"},{"location":"test-cluster-autoscaler/#check-logs","text":"kubectl -n kube-system logs deployment.apps/cluster-autoscaler","title":"Check logs"}]}