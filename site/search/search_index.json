{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction This tutorial describes how-to-steps to setup a simple EKS cluster. This tutorial will provide steps for following: How to setup a simple kubernetes cluster using eksctl How to add new nodegroups How to add autoscaler to dynamically scale in and out How to setup a stateless application Application Structure |\u2500\u2500 docs # Folder contains all the mkdocs files | \u251c\u2500\u2500 img # Contains all images referenced in mkdocs | \u251c\u2500\u2500 *.md # Other mkdocs .md files \u251c\u2500\u2500 mkdocs.yml # Yaml for for mkdocs \u251c\u2500\u2500 .gitattributes | |\u2500\u2500 cluster-ng # Folder containing a simple EKS cluster with on-demand instances | \u251c\u2500\u2500 eks-cluster.yaml # Yaml file for a simple cluster | \u251c\u2500\u2500 eks-cluster-cmds.md # List of commands to run |\u2500\u2500 cluster-ng-mixed # Folder containing an EKS cluster with spot & on-demand instances | \u251c\u2500\u2500 eks-cluster.yaml # Yaml file for cluster with different node groups |\u2500\u2500 cluster-autoscaler # Folder containing a EKS cluster with auto-scaler | \u251c\u2500\u2500 eks-cluster.yaml # Yaml file for a simple cluster | \u251c\u2500\u2500 eks-cluster-cmds.md # List of commands to run |\u2500\u2500 cluster-stateless-app # Folder containing a simple stateless example from kubernetes.io | \u251c\u2500\u2500 frontend-deployment.yaml # Yaml file for frontend deployment | \u251c\u2500\u2500 frontend-service.yaml # Yaml file for frontend service | \u251c\u2500\u2500 redis-leader-deployment.yaml # Yaml file for frontend deployment | \u251c\u2500\u2500 redis-leader-service.yaml # Yaml file for frontend service | \u251c\u2500\u2500 redis-followers-deployment.yaml # Yaml file for frontend deployment | \u251c\u2500\u2500 redis-follower-service.yaml # Yaml file for frontend service | \u251c\u2500\u2500 cluster-stateless-app-cmds.md # List of commands to run \u251c\u2500\u2500 README.md # Standard README.md file Credits Although much of the material has been sources from various sources on the web ( including ChatGPT ), I did find this Udemy tutorial very useful in crafting these how-to-steps : Amazon EKS Starter: Docker on AWS EKS with Kubernetes","title":"Introduction"},{"location":"#introduction","text":"This tutorial describes how-to-steps to setup a simple EKS cluster. This tutorial will provide steps for following: How to setup a simple kubernetes cluster using eksctl How to add new nodegroups How to add autoscaler to dynamically scale in and out How to setup a stateless application","title":"Introduction"},{"location":"#application-structure","text":"|\u2500\u2500 docs # Folder contains all the mkdocs files | \u251c\u2500\u2500 img # Contains all images referenced in mkdocs | \u251c\u2500\u2500 *.md # Other mkdocs .md files \u251c\u2500\u2500 mkdocs.yml # Yaml for for mkdocs \u251c\u2500\u2500 .gitattributes | |\u2500\u2500 cluster-ng # Folder containing a simple EKS cluster with on-demand instances | \u251c\u2500\u2500 eks-cluster.yaml # Yaml file for a simple cluster | \u251c\u2500\u2500 eks-cluster-cmds.md # List of commands to run |\u2500\u2500 cluster-ng-mixed # Folder containing an EKS cluster with spot & on-demand instances | \u251c\u2500\u2500 eks-cluster.yaml # Yaml file for cluster with different node groups |\u2500\u2500 cluster-autoscaler # Folder containing a EKS cluster with auto-scaler | \u251c\u2500\u2500 eks-cluster.yaml # Yaml file for a simple cluster | \u251c\u2500\u2500 eks-cluster-cmds.md # List of commands to run |\u2500\u2500 cluster-stateless-app # Folder containing a simple stateless example from kubernetes.io | \u251c\u2500\u2500 frontend-deployment.yaml # Yaml file for frontend deployment | \u251c\u2500\u2500 frontend-service.yaml # Yaml file for frontend service | \u251c\u2500\u2500 redis-leader-deployment.yaml # Yaml file for frontend deployment | \u251c\u2500\u2500 redis-leader-service.yaml # Yaml file for frontend service | \u251c\u2500\u2500 redis-followers-deployment.yaml # Yaml file for frontend deployment | \u251c\u2500\u2500 redis-follower-service.yaml # Yaml file for frontend service | \u251c\u2500\u2500 cluster-stateless-app-cmds.md # List of commands to run \u251c\u2500\u2500 README.md # Standard README.md file","title":"Application Structure"},{"location":"#credits","text":"Although much of the material has been sources from various sources on the web ( including ChatGPT ), I did find this Udemy tutorial very useful in crafting these how-to-steps : Amazon EKS Starter: Docker on AWS EKS with Kubernetes","title":"Credits"},{"location":"clean-up/","text":"Cleaning Up For clean up, all applications are undeployed using kubectl delete -f <app-deployment-file> kubectl delete -f <app-service-file> Get all nodegroups ksctl get nodegroup --cluster=eks-cluster-ng PS C:\\Users\\aniru\\workspace\\github\\aws-eks\\cluster-stateless-app> eksctl get nodegroup --cluster=eks-cluster-ngCLUSTER NODEGROUP STATUS CREATED MIN SIZE MAX SIZE DESIRED CAPACITY INSTANCE TYPE IMAGE ID ASG NAME TYPE eks-cluster-ng scale-east1b CREATE_COMPLETE 2023-02-14T12:10:31Z 1 2 1 t2.nano ami-0d4bdb1cf2f07d811 eksctl-eks-cluster-ng-nodegroup-scale-east1b-NodeGroup-PVRF5CRPR3DU unmanaged eks-cluster-ng scale-east1c CREATE_COMPLETE 2023-02-14T14:39:47Z 1 3 3 t2.micro ami-0d4bdb1cf2f07d811 eksctl-eks-cluster-ng-nodegroup-scale-east1c-NodeGroup-K1ALBP32WELN unmanaged eks-cluster-ng scale-spot CREATE_COMPLETE 2023-02-14T12:10:32Z 1 4 2 t2.micro ami-0d4bdb1cf2f07d811 eksctl-eks-cluster-ng-nodegroup-scale-spot-NodeGroup-1SZS5D6OYGK21 unmanaged First delete all nodegroups eksctl delete nodegroup --cluster=eks-cluster-ng --name=scale-east1b eksctl delete nodegroup --cluster=eks-cluster-ng --name=scale-east1c eksctl delete nodegroup --cluster=eks-cluster-ng --name=scale-spot Now delete the cluster eksctl delete cluster --name=eks-cluster-ng","title":"Clean up"},{"location":"clean-up/#cleaning-up","text":"For clean up, all applications are undeployed using kubectl delete -f <app-deployment-file> kubectl delete -f <app-service-file> Get all nodegroups ksctl get nodegroup --cluster=eks-cluster-ng PS C:\\Users\\aniru\\workspace\\github\\aws-eks\\cluster-stateless-app> eksctl get nodegroup --cluster=eks-cluster-ngCLUSTER NODEGROUP STATUS CREATED MIN SIZE MAX SIZE DESIRED CAPACITY INSTANCE TYPE IMAGE ID ASG NAME TYPE eks-cluster-ng scale-east1b CREATE_COMPLETE 2023-02-14T12:10:31Z 1 2 1 t2.nano ami-0d4bdb1cf2f07d811 eksctl-eks-cluster-ng-nodegroup-scale-east1b-NodeGroup-PVRF5CRPR3DU unmanaged eks-cluster-ng scale-east1c CREATE_COMPLETE 2023-02-14T14:39:47Z 1 3 3 t2.micro ami-0d4bdb1cf2f07d811 eksctl-eks-cluster-ng-nodegroup-scale-east1c-NodeGroup-K1ALBP32WELN unmanaged eks-cluster-ng scale-spot CREATE_COMPLETE 2023-02-14T12:10:32Z 1 4 2 t2.micro ami-0d4bdb1cf2f07d811 eksctl-eks-cluster-ng-nodegroup-scale-spot-NodeGroup-1SZS5D6OYGK21 unmanaged First delete all nodegroups eksctl delete nodegroup --cluster=eks-cluster-ng --name=scale-east1b eksctl delete nodegroup --cluster=eks-cluster-ng --name=scale-east1c eksctl delete nodegroup --cluster=eks-cluster-ng --name=scale-spot Now delete the cluster eksctl delete cluster --name=eks-cluster-ng","title":"Cleaning Up"},{"location":"create-cluster-autoscaler/","text":"Create cluster with autoscaler In this section, we will create a autoscaler that allows us to dynamically scale the nodes within a nodegroup - in and out. We will start by adding 3 new nodegroups to our existing cluster eks-cluster-ng : scale-east1b : Represents on-demand instances of one instance type running on a single AZ for stateful workloads scale-east1c : Represents on-demand instances of multiple instance types running on a single AZ for stateful workloads scale-spot : represents spot instances running on multi AZ for stateless workloads The cluster auto-scaler automatically launches additional worker nodes if more resources are needed, and shutdowns worker nodes if they are under-utilized. The autoscaling works within a nodegroup, hence we will create a nodegroup first which has this feature enabled. Create folder cluster-autoscaler . Copy the following contents into file eks-cluster.yaml : piVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: eks-cluster-ng region: us-east-1 nodeGroups: - name: scale-east1b instanceType: t2.nano desiredCapacity: 1 maxSize: 2 availabilityZones: [\"us-east-1b\"] iam: withAddonPolicies: autoScaler: true labels: nodegroup-type: stateful-east1b instance-type: onDemand - name: scale-east1c instanceType: t2.micro desiredCapacity: 1 maxSize: 3 availabilityZones: [\"us-east-1c\"] iam: withAddonPolicies: autoScaler: true labels: nodegroup-type: stateful-east1c instance-type: onDemand ssh: # use existing EC2 key publicKeyName: eks-course - name: scale-spot desiredCapacity: 1 maxSize: 4 instancesDistribution: instanceTypes: [\"t2.micro\", \"t2.small\"] onDemandBaseCapacity: 0 onDemandPercentageAboveBaseCapacity: 0 availabilityZones: [\"us-east-1b\",\"us-east-1c\"] iam: withAddonPolicies: autoScaler: true labels: nodegroup-type: stateless-workload instance-type: spot ssh: publicKeyName: eks-course availabilityZones: [\"us-east-1b\",\"us-east-1c\"] This will tell EKS to : Apply additional cluster configurations to cluster eks-cluster-ng Apply defintions for 3 nodegroups : scale-east1b : On-demand instances in us-east-1b scale-east1c : On-demand instances in us-east-1c scale-spot : Spot instances in us-east-1b, us-east-1c Note the autoScaler property is set to true Create cluster nodegroups We will now create the new nodegroups in our existing cluster eksctl create nodegroup --config-file=eks-cluster.yaml kubectl get nodes PS C:\\Users\\aniru\\workspace\\github\\aws-eks\\cluster-autoscaler> kubectl get nodes NAME STATUS ROLES AGE VERSION ip-192-168-14-220.ec2.internal Ready <none> 2m7s v1.23.15-eks-49d8fe8 ip-192-168-20-253.ec2.internal Ready <none> 4h51m v1.23.15-eks-49d8fe8 ip-192-168-33-152.ec2.internal Ready <none> 4h51m v1.23.15-eks-49d8fe8 ip-192-168-53-222.ec2.internal Ready <none> 78s v1.23.15-eks-49d8fe8 ip-192-168-61-217.ec2.internal Ready <none> 2m56s v1.23.15-eks-49d8fe8 Check the AGE column to see which of the instances were started. Now let's check the nodegroups using : eksctl get nodegroup --cluster=eks-cluster-ng PS C:\\Users\\aniru\\workspace\\github\\aws-eks\\cluster-autoscaler> eksctl get nodegroup --cluster=eks-cluster-ng CLUSTER NODEGROUP STATUS CREATED MIN SIZE MAX SIZE DESIRED CAPACITY INSTANCE TYPE IMAGE ID ASG NAME TYPE eks-cluster-ng ng-1 CREATE_COMPLETE 2023-02-14T07:23:00Z 2 2 2 t2.nano ami-0d4bdb1cf2f07d811 eksctl-eks-cluster-ng-nodegroup-ng-1-NodeGroup-1LRT8H4ZKAVO9 unmanaged eks-cluster-ng scale-east1b CREATE_COMPLETE 2023-02-14T12:10:31Z 1 2 1 t2.nano ami-0d4bdb1cf2f07d811 eksctl-eks-cluster-ng-nodegroup-scale-east1b-NodeGroup-PVRF5CRPR3DU unmanaged eks-cluster-ng scale-east1c CREATE_COMPLETE 2023-02-14T12:10:31Z 1 3 1 t2.nano ami-0d4bdb1cf2f07d811 eksctl-eks-cluster-ng-nodegroup-scale-east1c-NodeGroup-15I9RHLLNMPQR unmanaged eks-cluster-ng scale-spot CREATE_COMPLETE 2023-02-14T12:10:32Z 1 4 1 t2.micro ami-0d4bdb1cf2f07d811 eksctl-eks-cluster-ng-nodegroup-scale-spot-NodeGroup-1SZS5D6OYGK21 unmanaged Let's delete the starter nodegroup ng-1 eksctl delete nodegroup --cluster=eks-cluster-ng --name='ng-1' PS C:\\Users\\aniru\\workspace\\github\\aws-eks\\cluster-autoscaler> eksctl delete nodegroup --cluster=eks-cluster-ng --name='ng-1' 2023-02-14 07:31:27 [\u2139] 1 nodegroup (ng-1) was included (based on the include/exclude rules) 2023-02-14 07:31:28 [\u2139] will drain 1 nodegroup(s) in cluster \"eks-cluster-ng\" 2023-02-14 07:31:28 [\u2139] starting parallel draining, max in-flight of 1 2023-02-14 07:31:31 [\u2139] cordon node \"ip-192-168-20-253.ec2.internal\" 2023-02-14 07:31:31 [\u2139] cordon node \"ip-192-168-33-152.ec2.internal\" Create deployment for auto-scaler Deploy the auto-scaler itself: kubectl apply -f https://raw.githubusercontent.com/kubernetes/autoscaler/master/cluster-autoscaler/cloudprovider/aws/examples/cluster-autoscaler-autodiscover.yaml PS C:\\Users\\aniru\\workspace\\github\\aws-eks\\cluster-autoscaler> kubectl apply -f https://raw.githubusercontent.com/kubernetes/autoscaler/master/cluster-autoscaler/cloudprovider/aws/examples/cluster-autoscaler-autodiscover.yaml serviceaccount/cluster-autoscaler created clusterrole.rbac.authorization.k8s.io/cluster-autoscaler created role.rbac.authorization.k8s.io/cluster-autoscaler created clusterrolebinding.rbac.authorization.k8s.io/cluster-autoscaler created rolebinding.rbac.authorization.k8s.io/cluster-autoscaler created deployment.apps/cluster-autoscaler created Add annotation to the deployment This prevents from being evicted kubectl -n kube-system annotate deployment.apps/cluster-autoscaler cluster-autoscaler.kubernetes.io/safe-to-evict=\"false\" PS C:\\Users\\aniru\\workspace\\github\\aws-eks\\cluster-autoscaler> kubectl -n kube-system annotate deployment.apps/cluster-autoscaler cluster-autoscaler.kubernetes.io/safe-to-evict=\"false\" deployment.apps/cluster-autoscaler annotated Set image version and cluster name We will now set matching image version and cluster name eks-cluster-ng in the deployment. Get the autoscaler image version: Open Kubernetes/Autoscalar Releases and get the latest release version matching your Kubernetes version, e.g. Kubernetes 1.14 => check for 1.14.n where \"n\" is the latest release version Edit deployment and set your EKS cluster name: kubectl -n kube-system edit deployment.apps/cluster-autoscaler set the image version at property image=k8s.gcr.io/cluster-autoscaler:vx.yy.z image: registry.k8s.io/autoscaling/cluster-autoscaler:v1.23.0 set your EKS cluster name at the end of property - --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled,k8s.io/cluster-autoscaler/<<EKS cluster name>> - --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled,k8s.io/cluster-autoscaler/eks-cluster-ng kubectl -n kube-system describe deployment cluster-autoscaler","title":"Create cluster with autoscaler"},{"location":"create-cluster-autoscaler/#create-cluster-with-autoscaler","text":"In this section, we will create a autoscaler that allows us to dynamically scale the nodes within a nodegroup - in and out. We will start by adding 3 new nodegroups to our existing cluster eks-cluster-ng : scale-east1b : Represents on-demand instances of one instance type running on a single AZ for stateful workloads scale-east1c : Represents on-demand instances of multiple instance types running on a single AZ for stateful workloads scale-spot : represents spot instances running on multi AZ for stateless workloads The cluster auto-scaler automatically launches additional worker nodes if more resources are needed, and shutdowns worker nodes if they are under-utilized. The autoscaling works within a nodegroup, hence we will create a nodegroup first which has this feature enabled. Create folder cluster-autoscaler . Copy the following contents into file eks-cluster.yaml : piVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: eks-cluster-ng region: us-east-1 nodeGroups: - name: scale-east1b instanceType: t2.nano desiredCapacity: 1 maxSize: 2 availabilityZones: [\"us-east-1b\"] iam: withAddonPolicies: autoScaler: true labels: nodegroup-type: stateful-east1b instance-type: onDemand - name: scale-east1c instanceType: t2.micro desiredCapacity: 1 maxSize: 3 availabilityZones: [\"us-east-1c\"] iam: withAddonPolicies: autoScaler: true labels: nodegroup-type: stateful-east1c instance-type: onDemand ssh: # use existing EC2 key publicKeyName: eks-course - name: scale-spot desiredCapacity: 1 maxSize: 4 instancesDistribution: instanceTypes: [\"t2.micro\", \"t2.small\"] onDemandBaseCapacity: 0 onDemandPercentageAboveBaseCapacity: 0 availabilityZones: [\"us-east-1b\",\"us-east-1c\"] iam: withAddonPolicies: autoScaler: true labels: nodegroup-type: stateless-workload instance-type: spot ssh: publicKeyName: eks-course availabilityZones: [\"us-east-1b\",\"us-east-1c\"] This will tell EKS to : Apply additional cluster configurations to cluster eks-cluster-ng Apply defintions for 3 nodegroups : scale-east1b : On-demand instances in us-east-1b scale-east1c : On-demand instances in us-east-1c scale-spot : Spot instances in us-east-1b, us-east-1c Note the autoScaler property is set to true","title":"Create cluster with autoscaler"},{"location":"create-cluster-autoscaler/#create-cluster-nodegroups","text":"We will now create the new nodegroups in our existing cluster eksctl create nodegroup --config-file=eks-cluster.yaml kubectl get nodes PS C:\\Users\\aniru\\workspace\\github\\aws-eks\\cluster-autoscaler> kubectl get nodes NAME STATUS ROLES AGE VERSION ip-192-168-14-220.ec2.internal Ready <none> 2m7s v1.23.15-eks-49d8fe8 ip-192-168-20-253.ec2.internal Ready <none> 4h51m v1.23.15-eks-49d8fe8 ip-192-168-33-152.ec2.internal Ready <none> 4h51m v1.23.15-eks-49d8fe8 ip-192-168-53-222.ec2.internal Ready <none> 78s v1.23.15-eks-49d8fe8 ip-192-168-61-217.ec2.internal Ready <none> 2m56s v1.23.15-eks-49d8fe8 Check the AGE column to see which of the instances were started. Now let's check the nodegroups using : eksctl get nodegroup --cluster=eks-cluster-ng PS C:\\Users\\aniru\\workspace\\github\\aws-eks\\cluster-autoscaler> eksctl get nodegroup --cluster=eks-cluster-ng CLUSTER NODEGROUP STATUS CREATED MIN SIZE MAX SIZE DESIRED CAPACITY INSTANCE TYPE IMAGE ID ASG NAME TYPE eks-cluster-ng ng-1 CREATE_COMPLETE 2023-02-14T07:23:00Z 2 2 2 t2.nano ami-0d4bdb1cf2f07d811 eksctl-eks-cluster-ng-nodegroup-ng-1-NodeGroup-1LRT8H4ZKAVO9 unmanaged eks-cluster-ng scale-east1b CREATE_COMPLETE 2023-02-14T12:10:31Z 1 2 1 t2.nano ami-0d4bdb1cf2f07d811 eksctl-eks-cluster-ng-nodegroup-scale-east1b-NodeGroup-PVRF5CRPR3DU unmanaged eks-cluster-ng scale-east1c CREATE_COMPLETE 2023-02-14T12:10:31Z 1 3 1 t2.nano ami-0d4bdb1cf2f07d811 eksctl-eks-cluster-ng-nodegroup-scale-east1c-NodeGroup-15I9RHLLNMPQR unmanaged eks-cluster-ng scale-spot CREATE_COMPLETE 2023-02-14T12:10:32Z 1 4 1 t2.micro ami-0d4bdb1cf2f07d811 eksctl-eks-cluster-ng-nodegroup-scale-spot-NodeGroup-1SZS5D6OYGK21 unmanaged Let's delete the starter nodegroup ng-1 eksctl delete nodegroup --cluster=eks-cluster-ng --name='ng-1' PS C:\\Users\\aniru\\workspace\\github\\aws-eks\\cluster-autoscaler> eksctl delete nodegroup --cluster=eks-cluster-ng --name='ng-1' 2023-02-14 07:31:27 [\u2139] 1 nodegroup (ng-1) was included (based on the include/exclude rules) 2023-02-14 07:31:28 [\u2139] will drain 1 nodegroup(s) in cluster \"eks-cluster-ng\" 2023-02-14 07:31:28 [\u2139] starting parallel draining, max in-flight of 1 2023-02-14 07:31:31 [\u2139] cordon node \"ip-192-168-20-253.ec2.internal\" 2023-02-14 07:31:31 [\u2139] cordon node \"ip-192-168-33-152.ec2.internal\"","title":"Create cluster nodegroups"},{"location":"create-cluster-autoscaler/#create-deployment-for-auto-scaler","text":"Deploy the auto-scaler itself: kubectl apply -f https://raw.githubusercontent.com/kubernetes/autoscaler/master/cluster-autoscaler/cloudprovider/aws/examples/cluster-autoscaler-autodiscover.yaml PS C:\\Users\\aniru\\workspace\\github\\aws-eks\\cluster-autoscaler> kubectl apply -f https://raw.githubusercontent.com/kubernetes/autoscaler/master/cluster-autoscaler/cloudprovider/aws/examples/cluster-autoscaler-autodiscover.yaml serviceaccount/cluster-autoscaler created clusterrole.rbac.authorization.k8s.io/cluster-autoscaler created role.rbac.authorization.k8s.io/cluster-autoscaler created clusterrolebinding.rbac.authorization.k8s.io/cluster-autoscaler created rolebinding.rbac.authorization.k8s.io/cluster-autoscaler created deployment.apps/cluster-autoscaler created","title":"Create deployment for auto-scaler"},{"location":"create-cluster-autoscaler/#add-annotation-to-the-deployment","text":"This prevents from being evicted kubectl -n kube-system annotate deployment.apps/cluster-autoscaler cluster-autoscaler.kubernetes.io/safe-to-evict=\"false\" PS C:\\Users\\aniru\\workspace\\github\\aws-eks\\cluster-autoscaler> kubectl -n kube-system annotate deployment.apps/cluster-autoscaler cluster-autoscaler.kubernetes.io/safe-to-evict=\"false\" deployment.apps/cluster-autoscaler annotated","title":"Add annotation to the deployment"},{"location":"create-cluster-autoscaler/#set-image-version-and-cluster-name","text":"We will now set matching image version and cluster name eks-cluster-ng in the deployment. Get the autoscaler image version: Open Kubernetes/Autoscalar Releases and get the latest release version matching your Kubernetes version, e.g. Kubernetes 1.14 => check for 1.14.n where \"n\" is the latest release version Edit deployment and set your EKS cluster name: kubectl -n kube-system edit deployment.apps/cluster-autoscaler set the image version at property image=k8s.gcr.io/cluster-autoscaler:vx.yy.z image: registry.k8s.io/autoscaling/cluster-autoscaler:v1.23.0 set your EKS cluster name at the end of property - --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled,k8s.io/cluster-autoscaler/<<EKS cluster name>> - --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled,k8s.io/cluster-autoscaler/eks-cluster-ng kubectl -n kube-system describe deployment cluster-autoscaler","title":"Set image version and cluster name"},{"location":"create-cluster/","text":"Create simple Cluster In this section, we will create a simple cluster using eksctl Create Yaml file Create folder cluster-ng . The first step is to create a simple yaml file in this folder. Let's call it eks-cluster.yaml . Copy the following contents to it: apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: eks-cluster-ng region: us-east-1 nodeGroups: - name: ng-1 instanceType: t2.nano desiredCapacity: 2 ssh: # use existing EC2 key publicKeyName: eks-course This tells EKS to: Create a cluster eks-cluster-ng in region us-east-1 Call the name of the nodegroup : ng-1 Set the instance type as :t2.micro Set the desired capcity of instances as 2 Set ssh key : eks-course Create cluster Now we will create the cluster by specfying the file to use with flag f with command : eksctl create cluster eksctl create cluster -f eks-cluster.yaml This will take 10-15 minutes. Now log into AWS console and navigate to the Cloud Formation service. You will see that eksctl creates a cloud formation stack. Check the resources tab to glace through various resources being created. Check cluster Now let's check the EC2 console. You will see 2 instances : You can also see VPCs & Security groups created by cloud formation service : Let's use the eksctl get nodegroup command to get the details of the nodegroup we just created : eksctl get nodegroup --cluster eks-cluster-ng S C:\\Users\\aniru\\workspace\\github\\aws-eks> eksctl get nodegroup --cluster eks-cluster-ng CLUSTER NODEGROUP STATUS CREATED MIN SIZE MAX SIZE DESIRED CAPACITY INSTANCE TYPE IMAGE ID ASG NAME TYPE eks-cluster-ng ng-1 CREATE_COMPLETE 2023-01-25T00:22:41Z 2 2 2 t2.nano ami-03a30cc1dda93f173 eksctl-eks-cluster-ng-nodegroup-ng-1-NodeGroup-6387ICJKD3BS unmanaged Let's use the kubectl command to get the nodes : kubectl get nodes PS C:\\Users\\aniru\\workspace\\github\\aws-eks> kubectl get nodes Kubeconfig user entry is using deprecated API version client.authentication.k8s.io/v1alpha1. Run 'aws eks update-kubeconfig' to update. NAME STATUS ROLES AGE VERSION ip-192-168-21-105.ec2.internal Ready <none> 71m v1.22.15-eks-fb459a0 ip-192-168-54-187.ec2.internal Ready <none> 71m v1.22.15-eks-fb459a0","title":"Create cluster"},{"location":"create-cluster/#create-simple-cluster","text":"In this section, we will create a simple cluster using eksctl","title":"Create simple Cluster"},{"location":"create-cluster/#create-yaml-file","text":"Create folder cluster-ng . The first step is to create a simple yaml file in this folder. Let's call it eks-cluster.yaml . Copy the following contents to it: apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: eks-cluster-ng region: us-east-1 nodeGroups: - name: ng-1 instanceType: t2.nano desiredCapacity: 2 ssh: # use existing EC2 key publicKeyName: eks-course This tells EKS to: Create a cluster eks-cluster-ng in region us-east-1 Call the name of the nodegroup : ng-1 Set the instance type as :t2.micro Set the desired capcity of instances as 2 Set ssh key : eks-course","title":"Create Yaml file"},{"location":"create-cluster/#create-cluster","text":"Now we will create the cluster by specfying the file to use with flag f with command : eksctl create cluster eksctl create cluster -f eks-cluster.yaml This will take 10-15 minutes. Now log into AWS console and navigate to the Cloud Formation service. You will see that eksctl creates a cloud formation stack. Check the resources tab to glace through various resources being created.","title":"Create cluster"},{"location":"create-cluster/#check-cluster","text":"Now let's check the EC2 console. You will see 2 instances : You can also see VPCs & Security groups created by cloud formation service : Let's use the eksctl get nodegroup command to get the details of the nodegroup we just created : eksctl get nodegroup --cluster eks-cluster-ng S C:\\Users\\aniru\\workspace\\github\\aws-eks> eksctl get nodegroup --cluster eks-cluster-ng CLUSTER NODEGROUP STATUS CREATED MIN SIZE MAX SIZE DESIRED CAPACITY INSTANCE TYPE IMAGE ID ASG NAME TYPE eks-cluster-ng ng-1 CREATE_COMPLETE 2023-01-25T00:22:41Z 2 2 2 t2.nano ami-03a30cc1dda93f173 eksctl-eks-cluster-ng-nodegroup-ng-1-NodeGroup-6387ICJKD3BS unmanaged Let's use the kubectl command to get the nodes : kubectl get nodes PS C:\\Users\\aniru\\workspace\\github\\aws-eks> kubectl get nodes Kubeconfig user entry is using deprecated API version client.authentication.k8s.io/v1alpha1. Run 'aws eks update-kubeconfig' to update. NAME STATUS ROLES AGE VERSION ip-192-168-21-105.ec2.internal Ready <none> 71m v1.22.15-eks-fb459a0 ip-192-168-54-187.ec2.internal Ready <none> 71m v1.22.15-eks-fb459a0","title":"Check cluster"},{"location":"create-stateless-app/","text":"Create Stateless App In this section we will : Deploy backend resources Deploy frontend resources Scale pods up/down Perform chaos testing Setup sample guestbook app This example is based on Kubernetes Guestbook . The application consists of: A Redis backend with a single leader pod for Writes & multiple follower pods for Reads A frontend app ( Guestbook app ) in PHP loadbalanced to public using a load balancer Guestbook app reads load balanced to multiple redis followers (reads) Guestbook app writes directed towards single redis leader (writes) Backend Deployment Our configuration files for Redis leader/followers includes both Deployment and Services kinds. Deployments and Services are often used in tandem: Deployments working to define the desired state of the application and Services working to make sure communication between almost any kind of resource and the rest of the cluster is stable and adaptable. For the purpose of this application we will scale our nodegroup scale-spot to 3 instances PS C:\\Users\\aniru\\workspace\\github\\aws-eks\\cluster-stateless-app> eksctl scale nodegroup --cluster=eks-cluster-ng --nodes=3 --nodes-max=4 --name=scale-spot 2023-02-14 10:22:58 [\u2139] scaling nodegroup \"scale-spot\" in cluster eks-cluster-ng 2023-02-14 10:23:02 [\u2139] nodegroup successfully scaled Redis leader For the Redis leader, first create the Deployment file redis-leader-deployment.yaml . Copy these contents to it : # SOURCE: https://cloud.google.com/kubernetes-engine/docs/tutorials/guestbook apiVersion: apps/v1 kind: Deployment metadata: name: redis-leader labels: app: redis role: leader tier: backend spec: replicas: 1 selector: matchLabels: app: redis template: metadata: labels: app: redis role: leader tier: backend spec: containers: - name: leader image: \"docker.io/redis:6.0.5\" resources: requests: cpu: 100m memory: 100Mi ports: - containerPort: 6379 This tells EKS : The deployment has replica of 1 Specifies the source of image Set resource requirement for cpu & memory Set the container port to 6379 Now let's create the Service on top of your deployment. Create file redis-leader-service.yaml and copy the following contents to it: # SOURCE: https://cloud.google.com/kubernetes-engine/docs/tutorials/guestbook apiVersion: v1 kind: Service metadata: name: redis-leader labels: app: redis role: leader tier: backend spec: ports: - port: 6379 targetPort: 6379 selector: app: redis role: leader tier: backend Notice that the metadata name and labels are same as the redis-leader Deployment. Here the target port that is exposed is set to 6379. Now that the configuration files are defined, let's deploy the Redis leader pod : kubectl apply -f redis-leader-deployment.yaml Let's test by checking if the pods were created kubectl get pods PS C:\\Users\\aniru\\workspace\\github\\aws-eks\\cluster-stateless-app> kubectl get pod NAME READY STATUS RESTARTS AGE redis-leader-766465cd9c-wh94g 1/1 Running 0 46s test-autoscaler-6545fc8df4-7pktn 0/1 Pending 0 66m Now we apply Redis service on top of this : kubectl apply -f redis-leader-service.yaml Let's test using : kubectl get service redis-leader PS C:\\Users\\aniru\\workspace\\github\\aws-eks\\cluster-stateless-app> kubectl get service redis-leader NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE redis-leader ClusterIP 10.100.232.230 <none> 6379/TCP 11s Redis follower For the Redis follower, let's create the Deployment file redis-followers-deployment.yaml . Copy these contents to it : # SOURCE: https://cloud.google.com/kubernetes-engine/docs/tutorials/guestbook apiVersion: apps/v1 kind: Deployment metadata: name: redis-follower labels: app: redis role: follower tier: backend spec: replicas: 2 selector: matchLabels: app: redis template: metadata: labels: app: redis role: follower tier: backend spec: containers: - name: follower image: gcr.io/google_samples/gb-redis-follower:v2 resources: requests: cpu: 100m memory: 100Mi ports: - containerPort: 6379 Just like before we will now create the Service file as redis-follower-service.yaml . Copy the following contents to it : # SOURCE: https://cloud.google.com/kubernetes-engine/docs/tutorials/guestbook apiVersion: v1 kind: Service metadata: name: redis-follower labels: app: redis role: follower tier: backend spec: ports: # the port that this service should serve on - port: 6379 selector: app: redis role: follower tier: backend Notice the labels that match the Deployment file and the internal port that the redis follower service should run on. Now that the configuration files are defined, let's deploy the Redis follower pod : kubectl apply -f redis-followers-deployment.yaml kubectl apply -f redis-follower-service.yaml Let's check the status of Redis followers by running : kubectl get pods -o wide kubectl get service PS C:\\Users\\aniru\\workspace\\github\\aws-eks\\cluster-stateless-app> kubectl get service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.100.0.1 <none> 443/TCP 7h40m redis-follower ClusterIP 10.100.249.155 <none> 6379/TCP 41m redis-leader ClusterIP 10.100.232.230 <none> 6379/TCP 42m kubectl describe node <node-name> Frontend app Now we will deploy the frontend app that will consist of 3 replicas of the Guestbook App ELB LoadBalancer service Guestbook deployment We will start with defining the Deployment file for the frontend app. Create frontend-deployment.yaml and copy the following contents : # SOURCE: https://cloud.google.com/kubernetes-engine/docs/tutorials/guestbook apiVersion: apps/v1 kind: Deployment metadata: name: frontend spec: replicas: 3 selector: matchLabels: app: guestbook tier: frontend template: metadata: labels: app: guestbook tier: frontend spec: containers: - name: php-redis image: gcr.io/google_samples/gb-frontend:v5 env: - name: GET_HOSTS_FROM value: \"dns\" resources: requests: cpu: 100m memory: 100Mi ports: - containerPort: 80 This tells EKS to : Generate 3 replicas of the frontend app Specfiy image source Set the container resources Set the container port Let's apply the deployment kubectl apply -f frontend-deployment.yaml Check all the pods for the guestbook app kubectl get pods kubectl get pods -l app=guestbook kubectl get pods -l app=guestbook -l tier=frontend PS C:\\Users\\aniru\\workspace\\github\\aws-eks\\cluster-stateless-app> kubectl get pods -l app=guestbook -l tier=frontend NAME READY STATUS RESTARTS AGE frontend-57df59b89c-5rkzr 1/1 Running 0 28m frontend-57df59b89c-f26lm 1/1 Running 0 28m frontend-57df59b89c-s7n2s 1/1 Running 0 28m Loadbalancer service We will now apply the loadbalancer service that allows us to expose all the loadbalanced service to the public. Create file frontend-service.yaml and copy the following : # SOURCE: https://cloud.google.com/kubernetes-engine/docs/tutorials/guestbook apiVersion: v1 kind: Service metadata: name: frontend labels: app: guestbook tier: frontend spec: # if your cluster supports it, uncomment the following to automatically create # an external load-balanced IP for the frontend service. # type: LoadBalancer type: LoadBalancer ports: # the port that this service should serve on - port: 80 selector: app: guestbook tier: frontend Let's apply the loadbalanced service kubectl apply -f frontend-service.yaml Check all services kubectl get services kubectl get service frontend kubetctl get pods -o wide kubectl describe service frontend PS C:\\Users\\aniru\\workspace\\github\\aws-eks\\cluster-stateless-app> kubectl get service frontend NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE frontend LoadBalancer 10.100.141.207 ab8f47054a1bb400f8700134e4410f1c-1044573003.us-east-1.elb.amazonaws.com 80:30074/TCP 91s Finally check AWS mgmt console for the ELB which has been created ! Access from outside the cluster Grab the public DNS of the frontend service LoadBalancer (ELB): kubectl describe service frontend Copy the name and paste it into your browser !!! Scale Pods up and down We can scale pods up and down in the following ways: kubectl Update YAML file Using kubectl We can scale up and down using kubectl command. To scale up run the following command : kubectl scale deployment frontend --replicas=5 PS C:\\Users\\aniru\\workspace\\github\\aws-eks\\cluster-stateless-app> kubectl get pods NAME READY STATUS RESTARTS AGE frontend-57df59b89c-5rkzr 1/1 Running 0 37m frontend-57df59b89c-f26lm 1/1 Running 0 37m frontend-57df59b89c-rhwrv 0/1 Pending 0 5s frontend-57df59b89c-s7n2s 1/1 Running 0 37m frontend-57df59b89c-xxjll 1/1 Running 0 5s redis-follower-84fcc94dfc-2k2pn 1/1 Running 0 91m redis-follower-84fcc94dfc-k6d2t 1/1 Running 0 91m redis-leader-766465cd9c-wh94g 1/1 Running 0 93m test-autoscaler-59c78f9974-7jn9n 1/1 Running 0 76m Let's check the pods now kubectl get pods Now let us scale it down kubectl scale deployment frontend --replicas=3 PS C:\\Users\\aniru\\workspace\\github\\aws-eks\\cluster-stateless-app> kubectl get pods NAME READY STATUS RESTARTS AGE frontend-57df59b89c-5rkzr 1/1 Running 0 38m frontend-57df59b89c-f26lm 1/1 Running 0 38m frontend-57df59b89c-s7n2s 1/1 Running 0 38m redis-follower-84fcc94dfc-2k2pn 1/1 Running 0 92m redis-follower-84fcc94dfc-k6d2t 1/1 Running 0 92m redis-leader-766465cd9c-wh94g 1/1 Running 0 94m test-autoscaler-59c78f9974-7jn9n 1/1 Running 0 76m You can check pods are terminating by checking the pods kubectl get pods Using YAML file We can scale the pods by updating the replicas tag in our yaml files. Let's test this out by updating file redis-followers-deployment.yaml . Update the replicas count to 5 as follows : # SOURCE: https://cloud.google.com/kubernetes-engine/docs/tutorials/guestbook apiVersion: apps/v1 kind: Deployment metadata: name: frontend spec: replicas: 5 selector: matchLabels: app: guestbook tier: frontend template: metadata: labels: app: guestbook tier: frontend spec: containers: - name: php-redis image: gcr.io/google_samples/gb-frontend:v5 env: - name: GET_HOSTS_FROM value: \"dns\" resources: requests: cpu: 100m memory: 100Mi ports: - containerPort: 80 Let's apply the new deployment file kubectl apply -f redis-followers-deployment.yaml Let's check this by running kubectl get deployment redis-follower kubectl get pods PS C:\\Users\\aniru\\workspace\\github\\aws-eks\\cluster-stateless-app> kubectl get deployment redis-follower NAME READY UP-TO-DATE AVAILABLE AGE redis-follower 5/5 5 5 99m PS C:\\Users\\aniru\\workspace\\github\\aws-eks\\cluster-stateless-app> kubectl get pods NAME READY STATUS RESTARTS AGE frontend-57df59b89c-5rkzr 1/1 Running 0 42m frontend-57df59b89c-f26lm 1/1 Running 0 42m frontend-57df59b89c-s7n2s 1/1 Running 0 42m redis-follower-84fcc94dfc-2k2pn 1/1 Running 0 95m redis-follower-84fcc94dfc-5fvt6 1/1 Running 0 31s redis-follower-84fcc94dfc-k6d2t 1/1 Running 0 95m redis-follower-84fcc94dfc-s6mcd 1/1 Running 0 31s redis-follower-84fcc94dfc-v4bcx 1/1 Running 0 31s redis-leader-766465cd9c-wh94g 1/1 Running 0 98m test-autoscaler-59c78f9974-7jn9n 1/1 Running 0 80m Perform chaos testing We will demonstrate self-healing mechanism of K8s by: Kill pods Stop K8s worker node(s) kubectl get pods kubectl get nodes kubectl get pods -o wide Now let's delete one of front-end pods kubectl delete pod <frontend-pod> Now check for pods look a the age column. Kubernetes restarts the pod kubectl get pods -o wide PS C:\\Users\\aniru\\workspace\\github\\aws-eks\\cluster-stateless-app> kubectl delete pod redis-follower-84fcc94dfc-k6d2t pod \"redis-follower-84fcc94dfc-k6d2t\" deleted PS C:\\Users\\aniru\\workspace\\github\\aws-eks\\cluster-stateless-app> kubectl get pods NAME READY STATUS RESTARTS AGE frontend-57df59b89c-5rkzr 1/1 Running 0 54m frontend-57df59b89c-f26lm 1/1 Running 0 54m frontend-57df59b89c-s7n2s 1/1 Running 0 54m redis-follower-84fcc94dfc-2k2pn 1/1 Running 0 107m redis-follower-84fcc94dfc-4wn4v 1/1 Running 0 87s redis-follower-84fcc94dfc-5fvt6 1/1 Running 0 12m redis-follower-84fcc94dfc-s6mcd 1/1 Running 0 12m redis-follower-84fcc94dfc-v4bcx 1/1 Running 0 12m redis-leader-766465cd9c-wh94g 1/1 Running 0 110m test-autoscaler-59c78f9974-7jn9n 1/1 Running 0 92m Now let's use AWS console to stop any instance ( AWS > EC2 > instances). You will notice 2 things : * Kubernetes will try to recover the nodes * Kubernetes will reshuffle the nodes if required Cleaning up Run the following commands to save costs kubectl delete deployment -l app=redis kubectl delete service -l app=redis kubectl delete deployment frontend kubectl delete service frontend PS C:\\Users\\aniru\\workspace\\github\\aws-eks\\cluster-stateless-app> kubectl get pods NAME READY STATUS RESTARTS AGE test-autoscaler-59c78f9974-7jn9n 1/1 Running 0 97m","title":"Create stateless app"},{"location":"create-stateless-app/#create-stateless-app","text":"In this section we will : Deploy backend resources Deploy frontend resources Scale pods up/down Perform chaos testing","title":"Create Stateless App"},{"location":"create-stateless-app/#setup-sample-guestbook-app","text":"This example is based on Kubernetes Guestbook . The application consists of: A Redis backend with a single leader pod for Writes & multiple follower pods for Reads A frontend app ( Guestbook app ) in PHP loadbalanced to public using a load balancer Guestbook app reads load balanced to multiple redis followers (reads) Guestbook app writes directed towards single redis leader (writes)","title":"Setup sample guestbook app"},{"location":"create-stateless-app/#backend-deployment","text":"Our configuration files for Redis leader/followers includes both Deployment and Services kinds. Deployments and Services are often used in tandem: Deployments working to define the desired state of the application and Services working to make sure communication between almost any kind of resource and the rest of the cluster is stable and adaptable. For the purpose of this application we will scale our nodegroup scale-spot to 3 instances PS C:\\Users\\aniru\\workspace\\github\\aws-eks\\cluster-stateless-app> eksctl scale nodegroup --cluster=eks-cluster-ng --nodes=3 --nodes-max=4 --name=scale-spot 2023-02-14 10:22:58 [\u2139] scaling nodegroup \"scale-spot\" in cluster eks-cluster-ng 2023-02-14 10:23:02 [\u2139] nodegroup successfully scaled","title":"Backend Deployment"},{"location":"create-stateless-app/#redis-leader","text":"For the Redis leader, first create the Deployment file redis-leader-deployment.yaml . Copy these contents to it : # SOURCE: https://cloud.google.com/kubernetes-engine/docs/tutorials/guestbook apiVersion: apps/v1 kind: Deployment metadata: name: redis-leader labels: app: redis role: leader tier: backend spec: replicas: 1 selector: matchLabels: app: redis template: metadata: labels: app: redis role: leader tier: backend spec: containers: - name: leader image: \"docker.io/redis:6.0.5\" resources: requests: cpu: 100m memory: 100Mi ports: - containerPort: 6379 This tells EKS : The deployment has replica of 1 Specifies the source of image Set resource requirement for cpu & memory Set the container port to 6379 Now let's create the Service on top of your deployment. Create file redis-leader-service.yaml and copy the following contents to it: # SOURCE: https://cloud.google.com/kubernetes-engine/docs/tutorials/guestbook apiVersion: v1 kind: Service metadata: name: redis-leader labels: app: redis role: leader tier: backend spec: ports: - port: 6379 targetPort: 6379 selector: app: redis role: leader tier: backend Notice that the metadata name and labels are same as the redis-leader Deployment. Here the target port that is exposed is set to 6379. Now that the configuration files are defined, let's deploy the Redis leader pod : kubectl apply -f redis-leader-deployment.yaml Let's test by checking if the pods were created kubectl get pods PS C:\\Users\\aniru\\workspace\\github\\aws-eks\\cluster-stateless-app> kubectl get pod NAME READY STATUS RESTARTS AGE redis-leader-766465cd9c-wh94g 1/1 Running 0 46s test-autoscaler-6545fc8df4-7pktn 0/1 Pending 0 66m Now we apply Redis service on top of this : kubectl apply -f redis-leader-service.yaml Let's test using : kubectl get service redis-leader PS C:\\Users\\aniru\\workspace\\github\\aws-eks\\cluster-stateless-app> kubectl get service redis-leader NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE redis-leader ClusterIP 10.100.232.230 <none> 6379/TCP 11s","title":"Redis leader"},{"location":"create-stateless-app/#redis-follower","text":"For the Redis follower, let's create the Deployment file redis-followers-deployment.yaml . Copy these contents to it : # SOURCE: https://cloud.google.com/kubernetes-engine/docs/tutorials/guestbook apiVersion: apps/v1 kind: Deployment metadata: name: redis-follower labels: app: redis role: follower tier: backend spec: replicas: 2 selector: matchLabels: app: redis template: metadata: labels: app: redis role: follower tier: backend spec: containers: - name: follower image: gcr.io/google_samples/gb-redis-follower:v2 resources: requests: cpu: 100m memory: 100Mi ports: - containerPort: 6379 Just like before we will now create the Service file as redis-follower-service.yaml . Copy the following contents to it : # SOURCE: https://cloud.google.com/kubernetes-engine/docs/tutorials/guestbook apiVersion: v1 kind: Service metadata: name: redis-follower labels: app: redis role: follower tier: backend spec: ports: # the port that this service should serve on - port: 6379 selector: app: redis role: follower tier: backend Notice the labels that match the Deployment file and the internal port that the redis follower service should run on. Now that the configuration files are defined, let's deploy the Redis follower pod : kubectl apply -f redis-followers-deployment.yaml kubectl apply -f redis-follower-service.yaml Let's check the status of Redis followers by running : kubectl get pods -o wide kubectl get service PS C:\\Users\\aniru\\workspace\\github\\aws-eks\\cluster-stateless-app> kubectl get service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.100.0.1 <none> 443/TCP 7h40m redis-follower ClusterIP 10.100.249.155 <none> 6379/TCP 41m redis-leader ClusterIP 10.100.232.230 <none> 6379/TCP 42m kubectl describe node <node-name>","title":"Redis follower"},{"location":"create-stateless-app/#frontend-app","text":"Now we will deploy the frontend app that will consist of 3 replicas of the Guestbook App ELB LoadBalancer service","title":"Frontend app"},{"location":"create-stateless-app/#guestbook-deployment","text":"We will start with defining the Deployment file for the frontend app. Create frontend-deployment.yaml and copy the following contents : # SOURCE: https://cloud.google.com/kubernetes-engine/docs/tutorials/guestbook apiVersion: apps/v1 kind: Deployment metadata: name: frontend spec: replicas: 3 selector: matchLabels: app: guestbook tier: frontend template: metadata: labels: app: guestbook tier: frontend spec: containers: - name: php-redis image: gcr.io/google_samples/gb-frontend:v5 env: - name: GET_HOSTS_FROM value: \"dns\" resources: requests: cpu: 100m memory: 100Mi ports: - containerPort: 80 This tells EKS to : Generate 3 replicas of the frontend app Specfiy image source Set the container resources Set the container port Let's apply the deployment kubectl apply -f frontend-deployment.yaml Check all the pods for the guestbook app kubectl get pods kubectl get pods -l app=guestbook kubectl get pods -l app=guestbook -l tier=frontend PS C:\\Users\\aniru\\workspace\\github\\aws-eks\\cluster-stateless-app> kubectl get pods -l app=guestbook -l tier=frontend NAME READY STATUS RESTARTS AGE frontend-57df59b89c-5rkzr 1/1 Running 0 28m frontend-57df59b89c-f26lm 1/1 Running 0 28m frontend-57df59b89c-s7n2s 1/1 Running 0 28m","title":"Guestbook deployment"},{"location":"create-stateless-app/#loadbalancer-service","text":"We will now apply the loadbalancer service that allows us to expose all the loadbalanced service to the public. Create file frontend-service.yaml and copy the following : # SOURCE: https://cloud.google.com/kubernetes-engine/docs/tutorials/guestbook apiVersion: v1 kind: Service metadata: name: frontend labels: app: guestbook tier: frontend spec: # if your cluster supports it, uncomment the following to automatically create # an external load-balanced IP for the frontend service. # type: LoadBalancer type: LoadBalancer ports: # the port that this service should serve on - port: 80 selector: app: guestbook tier: frontend Let's apply the loadbalanced service kubectl apply -f frontend-service.yaml Check all services kubectl get services kubectl get service frontend kubetctl get pods -o wide kubectl describe service frontend PS C:\\Users\\aniru\\workspace\\github\\aws-eks\\cluster-stateless-app> kubectl get service frontend NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE frontend LoadBalancer 10.100.141.207 ab8f47054a1bb400f8700134e4410f1c-1044573003.us-east-1.elb.amazonaws.com 80:30074/TCP 91s Finally check AWS mgmt console for the ELB which has been created !","title":"Loadbalancer service"},{"location":"create-stateless-app/#access-from-outside-the-cluster","text":"Grab the public DNS of the frontend service LoadBalancer (ELB): kubectl describe service frontend Copy the name and paste it into your browser !!!","title":"Access from outside the cluster"},{"location":"create-stateless-app/#scale-pods-up-and-down","text":"We can scale pods up and down in the following ways: kubectl Update YAML file","title":"Scale Pods up and down"},{"location":"create-stateless-app/#using-kubectl","text":"We can scale up and down using kubectl command. To scale up run the following command : kubectl scale deployment frontend --replicas=5 PS C:\\Users\\aniru\\workspace\\github\\aws-eks\\cluster-stateless-app> kubectl get pods NAME READY STATUS RESTARTS AGE frontend-57df59b89c-5rkzr 1/1 Running 0 37m frontend-57df59b89c-f26lm 1/1 Running 0 37m frontend-57df59b89c-rhwrv 0/1 Pending 0 5s frontend-57df59b89c-s7n2s 1/1 Running 0 37m frontend-57df59b89c-xxjll 1/1 Running 0 5s redis-follower-84fcc94dfc-2k2pn 1/1 Running 0 91m redis-follower-84fcc94dfc-k6d2t 1/1 Running 0 91m redis-leader-766465cd9c-wh94g 1/1 Running 0 93m test-autoscaler-59c78f9974-7jn9n 1/1 Running 0 76m Let's check the pods now kubectl get pods Now let us scale it down kubectl scale deployment frontend --replicas=3 PS C:\\Users\\aniru\\workspace\\github\\aws-eks\\cluster-stateless-app> kubectl get pods NAME READY STATUS RESTARTS AGE frontend-57df59b89c-5rkzr 1/1 Running 0 38m frontend-57df59b89c-f26lm 1/1 Running 0 38m frontend-57df59b89c-s7n2s 1/1 Running 0 38m redis-follower-84fcc94dfc-2k2pn 1/1 Running 0 92m redis-follower-84fcc94dfc-k6d2t 1/1 Running 0 92m redis-leader-766465cd9c-wh94g 1/1 Running 0 94m test-autoscaler-59c78f9974-7jn9n 1/1 Running 0 76m You can check pods are terminating by checking the pods kubectl get pods","title":"Using kubectl"},{"location":"create-stateless-app/#using-yaml-file","text":"We can scale the pods by updating the replicas tag in our yaml files. Let's test this out by updating file redis-followers-deployment.yaml . Update the replicas count to 5 as follows : # SOURCE: https://cloud.google.com/kubernetes-engine/docs/tutorials/guestbook apiVersion: apps/v1 kind: Deployment metadata: name: frontend spec: replicas: 5 selector: matchLabels: app: guestbook tier: frontend template: metadata: labels: app: guestbook tier: frontend spec: containers: - name: php-redis image: gcr.io/google_samples/gb-frontend:v5 env: - name: GET_HOSTS_FROM value: \"dns\" resources: requests: cpu: 100m memory: 100Mi ports: - containerPort: 80 Let's apply the new deployment file kubectl apply -f redis-followers-deployment.yaml Let's check this by running kubectl get deployment redis-follower kubectl get pods PS C:\\Users\\aniru\\workspace\\github\\aws-eks\\cluster-stateless-app> kubectl get deployment redis-follower NAME READY UP-TO-DATE AVAILABLE AGE redis-follower 5/5 5 5 99m PS C:\\Users\\aniru\\workspace\\github\\aws-eks\\cluster-stateless-app> kubectl get pods NAME READY STATUS RESTARTS AGE frontend-57df59b89c-5rkzr 1/1 Running 0 42m frontend-57df59b89c-f26lm 1/1 Running 0 42m frontend-57df59b89c-s7n2s 1/1 Running 0 42m redis-follower-84fcc94dfc-2k2pn 1/1 Running 0 95m redis-follower-84fcc94dfc-5fvt6 1/1 Running 0 31s redis-follower-84fcc94dfc-k6d2t 1/1 Running 0 95m redis-follower-84fcc94dfc-s6mcd 1/1 Running 0 31s redis-follower-84fcc94dfc-v4bcx 1/1 Running 0 31s redis-leader-766465cd9c-wh94g 1/1 Running 0 98m test-autoscaler-59c78f9974-7jn9n 1/1 Running 0 80m","title":"Using YAML file"},{"location":"create-stateless-app/#perform-chaos-testing","text":"We will demonstrate self-healing mechanism of K8s by: Kill pods Stop K8s worker node(s) kubectl get pods kubectl get nodes kubectl get pods -o wide Now let's delete one of front-end pods kubectl delete pod <frontend-pod> Now check for pods look a the age column. Kubernetes restarts the pod kubectl get pods -o wide PS C:\\Users\\aniru\\workspace\\github\\aws-eks\\cluster-stateless-app> kubectl delete pod redis-follower-84fcc94dfc-k6d2t pod \"redis-follower-84fcc94dfc-k6d2t\" deleted PS C:\\Users\\aniru\\workspace\\github\\aws-eks\\cluster-stateless-app> kubectl get pods NAME READY STATUS RESTARTS AGE frontend-57df59b89c-5rkzr 1/1 Running 0 54m frontend-57df59b89c-f26lm 1/1 Running 0 54m frontend-57df59b89c-s7n2s 1/1 Running 0 54m redis-follower-84fcc94dfc-2k2pn 1/1 Running 0 107m redis-follower-84fcc94dfc-4wn4v 1/1 Running 0 87s redis-follower-84fcc94dfc-5fvt6 1/1 Running 0 12m redis-follower-84fcc94dfc-s6mcd 1/1 Running 0 12m redis-follower-84fcc94dfc-v4bcx 1/1 Running 0 12m redis-leader-766465cd9c-wh94g 1/1 Running 0 110m test-autoscaler-59c78f9974-7jn9n 1/1 Running 0 92m Now let's use AWS console to stop any instance ( AWS > EC2 > instances). You will notice 2 things : * Kubernetes will try to recover the nodes * Kubernetes will reshuffle the nodes if required","title":"Perform chaos testing"},{"location":"create-stateless-app/#cleaning-up","text":"Run the following commands to save costs kubectl delete deployment -l app=redis kubectl delete service -l app=redis kubectl delete deployment frontend kubectl delete service frontend PS C:\\Users\\aniru\\workspace\\github\\aws-eks\\cluster-stateless-app> kubectl get pods NAME READY STATUS RESTARTS AGE test-autoscaler-59c78f9974-7jn9n 1/1 Running 0 97m","title":"Cleaning up"},{"location":"extend-cluster/","text":"Extend Cluster In this section we will extend the cluster by adding a nodegroup that contains a mix of: On-demand instances Spot instances Define Yaml file Create folder cluster-ng-mixed . We will now extend the cluster by adding a nodegroup to file eks-cluster.yaml . The file adds a new nodegroup ng-mixed . Copy the following contents : apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: eks-cluster-ng region: us-east-1 nodeGroups: - name: ng-1 instanceType: t2.nano desiredCapacity: 2 ssh: # use existing EC2 key publicKeyName: eks-course - name: ng-mixed minSize: 3 maxSize: 5 instancesDistribution: maxPrice: 0.2 instanceTypes: [\"t2.small\", \"t3.small\"] onDemandBaseCapacity: 0 onDemandPercentageAboveBaseCapacity: 50 ssh: publicKeyName: eks-course This tells EKS to : Add a new nodegroup ng-mixed Set min & max size Define the instance types 50% of the instance types are on-demand and the rest on spot Add nodegroup First check your cluster is running : eksctl get cluster PS C:\\Users\\aniru\\workspace\\github\\aws-eks\\cluster-ng> eksctl get cluster NAME REGION EKSCTL CREATED eks-cluster-ng us-east-1 True Let's add the new nodegroup : eksctl create nodegroup --config-file=eks-cluster.yaml --include='ng-mixed' PS C:\\Users\\aniru\\workspace\\github\\aws-eks\\cluster-ng-mixed> eksctl create nodegroup --config-file=eks-cluster.yaml --include='ng-mixed' 2023-02-14 03:25:59 [\u2139] will use version 1.23 for new nodegroup(s) based on control plane version 2023-02-14 03:26:06 [\u2139] nodegroup \"ng-1\" will use \"ami-0d4bdb1cf2f07d811\" [AmazonLinux2/1.23] 2023-02-14 03:26:07 [\u2139] using EC2 key pair \"eks-course\" 2023-02-14 03:26:07 [\u2139] nodegroup \"ng-mixed\" will use \"ami-0d4bdb1cf2f07d811\" [AmazonLinux2/1.23] 2023-02-14 03:26:08 [\u2139] using EC2 key pair \"eks-course\" 2023-02-14 03:26:11 [\u2139] 1 existing nodegroup(s) (ng-1) will be excluded 2023-02-14 03:26:11 [\u2139] combined include rules: ng-mixed 2023-02-14 03:26:11 [\u2139] 1 nodegroup (ng-mixed) was included (based on the include/exclude rules) 2023-02-14 03:26:11 [\u2139] will create a CloudFormation stack for each of 1 nodegroups in cluster \"eks-cluster-ng\" Note line combined include rules: ng-mixed Also check the new nodegroup ng-mixed created after a few minutes by running below command. Check min size and max size : PS C:\\Users\\aniru\\workspace\\github\\aws-eks> eksctl get nodegroup --cluster=eks-cluster-ng CLUSTER NODEGROUP STATUS CREATED MIN SIZE MAX SIZE DESIRED CAPACITY INSTANCE TYPE IMAGE ID ASG NAME TYPE eks-cluster-ng ng-1 CREATE_COMPLETE 2023-02-14T07:23:00Z 2 2 2 t2.nano ami-0d4bdb1cf2f07d811 eksctl-eks-cluster-ng-nodegroup-ng-1-NodeGroup-1LRT8H4ZKAVO9 unmanaged eks-cluster-ng ng-mixed CREATE_COMPLETE 2023-02-14T08:26:13Z 3 5 3 t2.small ami-0d4bdb1cf2f07d811 eksctl-eks-cluster-ng-nodegroup-ng-mixed-NodeGroup-1I8YSGYWKO2O5 unmanaged You can also review this on AWS console : Delete nodegroup Now let's drain the ng-mixed nodegroup we just created so we don't incur additional costs : eksctl delete nodegroup --config-file=eks-cluster.yaml --include='ng-mixed' --approve The draining will usually take some time as is indicated by STATUS=DELETE_IN_PROGRESS : PS C:\\Users\\aniru\\workspace\\github\\aws-eks> eksctl get nodegroup --cluster=eks-cluster-ng CLUSTER NODEGROUP STATUS CREATED MIN SIZE MAX SIZE DESIRED CAPACITY INSTANCE TYPE IMAGE ID ASG NAME TYPE eks-cluster-ng ng-1 CREATE_COMPLETE 2023-02-14T07:23:00Z 2 2 2 t2.nano ami-0d4bdb1cf2f07d811 eksctl-eks-cluster-ng-nodegroup-ng-1-NodeGroup-1LRT8H4ZKAVO9 unmanaged eks-cluster-ng ng-mixed DELETE_IN_PROGRESS 2023-02-14T08:26:13Z 0 0 0 t2.small ami-0d4bdb1cf2f07d811 eksctl-eks-cluster-ng-nodegroup-ng-mixed-NodeGroup-1I8YSGYWKO2O5 unmanaged","title":"Extend cluster"},{"location":"extend-cluster/#extend-cluster","text":"In this section we will extend the cluster by adding a nodegroup that contains a mix of: On-demand instances Spot instances","title":"Extend Cluster"},{"location":"extend-cluster/#define-yaml-file","text":"Create folder cluster-ng-mixed . We will now extend the cluster by adding a nodegroup to file eks-cluster.yaml . The file adds a new nodegroup ng-mixed . Copy the following contents : apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: eks-cluster-ng region: us-east-1 nodeGroups: - name: ng-1 instanceType: t2.nano desiredCapacity: 2 ssh: # use existing EC2 key publicKeyName: eks-course - name: ng-mixed minSize: 3 maxSize: 5 instancesDistribution: maxPrice: 0.2 instanceTypes: [\"t2.small\", \"t3.small\"] onDemandBaseCapacity: 0 onDemandPercentageAboveBaseCapacity: 50 ssh: publicKeyName: eks-course This tells EKS to : Add a new nodegroup ng-mixed Set min & max size Define the instance types 50% of the instance types are on-demand and the rest on spot","title":"Define Yaml file"},{"location":"extend-cluster/#add-nodegroup","text":"First check your cluster is running : eksctl get cluster PS C:\\Users\\aniru\\workspace\\github\\aws-eks\\cluster-ng> eksctl get cluster NAME REGION EKSCTL CREATED eks-cluster-ng us-east-1 True Let's add the new nodegroup : eksctl create nodegroup --config-file=eks-cluster.yaml --include='ng-mixed' PS C:\\Users\\aniru\\workspace\\github\\aws-eks\\cluster-ng-mixed> eksctl create nodegroup --config-file=eks-cluster.yaml --include='ng-mixed' 2023-02-14 03:25:59 [\u2139] will use version 1.23 for new nodegroup(s) based on control plane version 2023-02-14 03:26:06 [\u2139] nodegroup \"ng-1\" will use \"ami-0d4bdb1cf2f07d811\" [AmazonLinux2/1.23] 2023-02-14 03:26:07 [\u2139] using EC2 key pair \"eks-course\" 2023-02-14 03:26:07 [\u2139] nodegroup \"ng-mixed\" will use \"ami-0d4bdb1cf2f07d811\" [AmazonLinux2/1.23] 2023-02-14 03:26:08 [\u2139] using EC2 key pair \"eks-course\" 2023-02-14 03:26:11 [\u2139] 1 existing nodegroup(s) (ng-1) will be excluded 2023-02-14 03:26:11 [\u2139] combined include rules: ng-mixed 2023-02-14 03:26:11 [\u2139] 1 nodegroup (ng-mixed) was included (based on the include/exclude rules) 2023-02-14 03:26:11 [\u2139] will create a CloudFormation stack for each of 1 nodegroups in cluster \"eks-cluster-ng\" Note line combined include rules: ng-mixed Also check the new nodegroup ng-mixed created after a few minutes by running below command. Check min size and max size : PS C:\\Users\\aniru\\workspace\\github\\aws-eks> eksctl get nodegroup --cluster=eks-cluster-ng CLUSTER NODEGROUP STATUS CREATED MIN SIZE MAX SIZE DESIRED CAPACITY INSTANCE TYPE IMAGE ID ASG NAME TYPE eks-cluster-ng ng-1 CREATE_COMPLETE 2023-02-14T07:23:00Z 2 2 2 t2.nano ami-0d4bdb1cf2f07d811 eksctl-eks-cluster-ng-nodegroup-ng-1-NodeGroup-1LRT8H4ZKAVO9 unmanaged eks-cluster-ng ng-mixed CREATE_COMPLETE 2023-02-14T08:26:13Z 3 5 3 t2.small ami-0d4bdb1cf2f07d811 eksctl-eks-cluster-ng-nodegroup-ng-mixed-NodeGroup-1I8YSGYWKO2O5 unmanaged You can also review this on AWS console :","title":"Add nodegroup"},{"location":"extend-cluster/#delete-nodegroup","text":"Now let's drain the ng-mixed nodegroup we just created so we don't incur additional costs : eksctl delete nodegroup --config-file=eks-cluster.yaml --include='ng-mixed' --approve The draining will usually take some time as is indicated by STATUS=DELETE_IN_PROGRESS : PS C:\\Users\\aniru\\workspace\\github\\aws-eks> eksctl get nodegroup --cluster=eks-cluster-ng CLUSTER NODEGROUP STATUS CREATED MIN SIZE MAX SIZE DESIRED CAPACITY INSTANCE TYPE IMAGE ID ASG NAME TYPE eks-cluster-ng ng-1 CREATE_COMPLETE 2023-02-14T07:23:00Z 2 2 2 t2.nano ami-0d4bdb1cf2f07d811 eksctl-eks-cluster-ng-nodegroup-ng-1-NodeGroup-1LRT8H4ZKAVO9 unmanaged eks-cluster-ng ng-mixed DELETE_IN_PROGRESS 2023-02-14T08:26:13Z 0 0 0 t2.small ami-0d4bdb1cf2f07d811 eksctl-eks-cluster-ng-nodegroup-ng-mixed-NodeGroup-1I8YSGYWKO2O5 unmanaged","title":"Delete nodegroup"},{"location":"local-environment/","text":"Local Enviornment Configurations Navigate to your AWS credentials file and update the default profile with access-keys of the newly created IAM user PS C:\\Users\\aniru\\.aws> cd ~\\.aws PS C:\\Users\\aniru\\.aws> ls Directory: C:\\Users\\aniru\\.aws Mode LastWriteTime Length Name ---- ------------- ------ ---- -a---- 11/1/2022 10:50 AM 99 config -a---- 1/24/2023 5:19 PM 461 credentials Edit credentials file by updating the [default] entry as follows: [default] aws_access_key_id=### aws_secret_access_key=### region=us-east-1 output=json","title":"Local environment"},{"location":"local-environment/#local-enviornment","text":"","title":"Local Enviornment"},{"location":"local-environment/#configurations","text":"Navigate to your AWS credentials file and update the default profile with access-keys of the newly created IAM user PS C:\\Users\\aniru\\.aws> cd ~\\.aws PS C:\\Users\\aniru\\.aws> ls Directory: C:\\Users\\aniru\\.aws Mode LastWriteTime Length Name ---- ------------- ------ ---- -a---- 11/1/2022 10:50 AM 99 config -a---- 1/24/2023 5:19 PM 461 credentials Edit credentials file by updating the [default] entry as follows: [default] aws_access_key_id=### aws_secret_access_key=### region=us-east-1 output=json","title":"Configurations"},{"location":"local-installations/","text":"Local Installations Install AWS CLI Follow the instructions from this page: Installing or updating the latest version of the AWS CLI Test PS C:\\Users\\aniru\\.aws> aws --version aws-cli/2.7.4 Python/3.9.11 Windows/10 exe/AMD64 prompt/off Install eksctl Follow the installation instructions from here : eksctl installation Test PS C:\\Users\\aniru\\.aws> eksctl version 0.99.0 Install kubectl Follow instructions from here: kubectl installation Test PS C:\\Users\\aniru\\.aws> kubectl version --client Client Version: version.Info{Major:\"1\", Minor:\"23\", GitVersion:\"v1.23.6\", GitCommit:\"ad3338546da947756e8a88aa6822e9c11e7eac22\", GitTreeState:\"clean\", BuildDate:\"2022-04-14T08:49:13Z\", GoVersion:\"go1.17.9\", Compiler:\"gc\", Platform:\"windows/amd64\"}","title":"Local installations"},{"location":"local-installations/#local-installations","text":"","title":"Local Installations"},{"location":"local-installations/#install-aws-cli","text":"Follow the instructions from this page: Installing or updating the latest version of the AWS CLI Test PS C:\\Users\\aniru\\.aws> aws --version aws-cli/2.7.4 Python/3.9.11 Windows/10 exe/AMD64 prompt/off","title":"Install AWS CLI"},{"location":"local-installations/#install-eksctl","text":"Follow the installation instructions from here : eksctl installation Test PS C:\\Users\\aniru\\.aws> eksctl version 0.99.0","title":"Install eksctl"},{"location":"local-installations/#install-kubectl","text":"Follow instructions from here: kubectl installation Test PS C:\\Users\\aniru\\.aws> kubectl version --client Client Version: version.Info{Major:\"1\", Minor:\"23\", GitVersion:\"v1.23.6\", GitCommit:\"ad3338546da947756e8a88aa6822e9c11e7eac22\", GitTreeState:\"clean\", BuildDate:\"2022-04-14T08:49:13Z\", GoVersion:\"go1.17.9\", Compiler:\"gc\", Platform:\"windows/amd64\"}","title":"Install kubectl"},{"location":"prerequisites/","text":"Prerequisites IAM user and permissions We will create a new IAM user ( eks-<user-name> ) for the purposes of the tutorial. Please keep in mind that you should never use your root account for working with AWS services. Create policies Create these 2 policies: EKS-Admin-policy: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"eks:*\" ], \"Resource\": \"*\" } ] } CloudFormation-Admin-policy: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"cloudformation:*\" ], \"Resource\": \"*\" } ] } Assign policies Finally, assign the following policies to your IAM user you are going to use throughout the course: AmazonEC2FullAccess IAMFullAccess AmazonVPCFullAccess CloudFormation-Admin-policy EKS-Admin-policy where the last 2 policies are the ones you created above Create IAM role Open https://console.aws.amazon.com/iam/ and choose Roles => create role Choose EKS service followed by Allows Amazon EKS to manage your clusters on your behalf Choose Next: Permissions Click Next: Review Enter a unique Role name, EKS-course-role and click Create Role Create key Pair Open EC2 dashboard https://console.aws.amazon.com/ec2 Click KeyPairs in left navigation bar under section \"Network&Security\" Click Create Key Pair Provide name for keypair, eks-course and click Create The keypair will be downloaded immediately => file eks-course.pem Create API Access key/secret Create key+secret via AWS console AWS-console => IAM => Users => => tab Security credentials => button Create access key","title":"Prerequisites"},{"location":"prerequisites/#prerequisites","text":"","title":"Prerequisites"},{"location":"prerequisites/#iam-user-and-permissions","text":"We will create a new IAM user ( eks-<user-name> ) for the purposes of the tutorial. Please keep in mind that you should never use your root account for working with AWS services.","title":"IAM user and permissions"},{"location":"prerequisites/#create-policies","text":"Create these 2 policies: EKS-Admin-policy: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"eks:*\" ], \"Resource\": \"*\" } ] } CloudFormation-Admin-policy: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"cloudformation:*\" ], \"Resource\": \"*\" } ] }","title":"Create policies"},{"location":"prerequisites/#assign-policies","text":"Finally, assign the following policies to your IAM user you are going to use throughout the course: AmazonEC2FullAccess IAMFullAccess AmazonVPCFullAccess CloudFormation-Admin-policy EKS-Admin-policy where the last 2 policies are the ones you created above","title":"Assign policies"},{"location":"prerequisites/#create-iam-role","text":"Open https://console.aws.amazon.com/iam/ and choose Roles => create role Choose EKS service followed by Allows Amazon EKS to manage your clusters on your behalf Choose Next: Permissions Click Next: Review Enter a unique Role name, EKS-course-role and click Create Role","title":"Create IAM role"},{"location":"prerequisites/#create-key-pair","text":"Open EC2 dashboard https://console.aws.amazon.com/ec2 Click KeyPairs in left navigation bar under section \"Network&Security\" Click Create Key Pair Provide name for keypair, eks-course and click Create The keypair will be downloaded immediately => file eks-course.pem","title":"Create key Pair"},{"location":"prerequisites/#create-api-access-keysecret","text":"Create key+secret via AWS console AWS-console => IAM => Users => => tab Security credentials => button Create access key","title":"Create API Access key/secret"},{"location":"test-cluster-autoscaler/","text":"Test Auto-scaler with Nginx deployment We will now test the auto-scaler and scale it up and down Create the deployment file In folder cluster-autoscaler , we will create a new deployment file nginx-deployment.yaml . Copy the following contents to it: apiVersion: apps/v1 kind: Deployment metadata: name: test-autoscaler spec: replicas: 1 selector: matchLabels: app: nginx template: metadata: labels: service: nginx app: nginx spec: containers: - image: nginx name: test-autoscaler resources: limits: cpu: 300m memory: 256Mi requests: cpu: 300m memory: 256Mi nodeSelector: instance-type: spot This will tell eksctl to : Refer to this one as of kind: Deployment This will have 1 replica with service: nginx and app: nginx Container will use image: nginx and will call it as name: test-autoscaler Specify resources with cpu & memory tags nodeSelector specifies what kind of instances we will need using tag instance-type: spot Deploy Nginx We will now apply the deployment file using kubectl command kubectl apply -f nginx-deployment.yaml Now check if the pod is running kubectl get pod S C:\\Users\\aniru\\workspace\\github\\aws-eks\\cluster-autoscaler> kubectl get pod NAME READY STATUS RESTARTS AGE test-autoscaler-6545fc8df4-7pktn 0/1 Pending 0 22s check the instance types kubectl get nodes -l instance-type=spot PS C:\\Users\\aniru\\workspace\\github\\aws-eks\\cluster-autoscaler> kubectl get nodes -l instance-type=spot NAME STATUS ROLES AGE VERSION ip-192-168-12-53.ec2.internal Ready <none> 8m5s v1.23.15-eks-49d8fe8 Scale the deployment Let's scale deployment to 3 replicas so we can test the autoscaler kubectl scale --replicas=3 deployment/test-autoscaler S C:\\Users\\aniru\\workspace\\github\\aws-eks\\cluster-autoscaler> kubectl scale --replicas=3 deployment/test-autoscaler deployment.apps/test-autoscaler scaled You should see 3 pods running now but using the same node kubectl get pod S C:\\Users\\aniru\\workspace\\github\\aws-eks\\cluster-autoscaler> kubectl get pod NAME READY STATUS RESTARTS AGE test-autoscaler-6545fc8df4-6qk5f 0/1 Pending 0 33s test-autoscaler-6545fc8df4-7pktn 0/1 Pending 0 6m12s test-autoscaler-6545fc8df4-hmrxd 0/1 Pending 0 33s kubectl get nodes -l instance-type=spot PS C:\\Users\\aniru\\workspace\\github\\aws-eks\\cluster-autoscaler> kubectl get nodes -l instance-type=spot NAME STATUS ROLES AGE VERSION ip-192-168-12-53.ec2.internal Ready <none> 15m v1.23.15-eks-49d8fe8 Scale-out the deployment Let's test scale-out by increasing the number of replicas to 4 kubectl scale --replicas=4 deployment/test-autoscaler Now check the pods kubectl get pods -o wide PS C:\\Users\\aniru\\workspace\\github\\aws-eks\\cluster-autoscaler> kubectl get pods -o wide --watch NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-autoscaler-6545fc8df4-6qk5f 0/1 Pending 0 4m39s <none> <none> <none> <none> test-autoscaler-6545fc8df4-7pktn 0/1 Pending 0 10m <none> <none> <none> <none> test-autoscaler-6545fc8df4-hmrxd 0/1 Pending 0 4m39s <none> <none> <none> <none> test-autoscaler-6545fc8df4-kfqg4 0/1 Pending 0 13s <none> <none> <none> <none> And let's check the # of instances. It should increase to accomodate the increased # of replicas kubectl get nodes -l instance-type=spot Now check page Spot Requests on AWS Console after a few minutes Scale-in the deployment Let's test scale-out by reducing the number of replicas to 1 kubectl scale --replicas=1 deployment/test-autoscaler Now check the pods kubectl get pod PS C:\\Users\\aniru\\workspace\\github\\aws-eks\\cluster-autoscaler> kubectl scale --replicas=1 deployment/test-autoscaler deployment.apps/test-autoscaler scaled PS C:\\Users\\aniru\\workspace\\github\\aws-eks\\cluster-autoscaler> kubectl get pod NAME READY STATUS RESTARTS AGE test-autoscaler-6545fc8df4-7pktn 0/1 Pending 0 21m And let's check the # of instances. It should increase to accomodate the increased # of replicas kubectl get nodes -l instance-type=spot Now check page EC2 > Instances > Spot Requests on AWS Console after a few minutes. Also check page EC2 > Auto Scaling Groups Check logs kubectl -n kube-system logs deployment.apps/cluster-autoscaler S C:\\Users\\aniru\\workspace\\github\\aws-eks\\cluster-autoscaler> kubectl -n kube-system logs deployment.apps/cluster-autoscaler Found 2 pods, using pod/cluster-autoscaler-c5bf8cc8-glhlr","title":"Test autoscaler with Nginx"},{"location":"test-cluster-autoscaler/#test-auto-scaler-with-nginx-deployment","text":"We will now test the auto-scaler and scale it up and down","title":"Test Auto-scaler with Nginx deployment"},{"location":"test-cluster-autoscaler/#create-the-deployment-file","text":"In folder cluster-autoscaler , we will create a new deployment file nginx-deployment.yaml . Copy the following contents to it: apiVersion: apps/v1 kind: Deployment metadata: name: test-autoscaler spec: replicas: 1 selector: matchLabels: app: nginx template: metadata: labels: service: nginx app: nginx spec: containers: - image: nginx name: test-autoscaler resources: limits: cpu: 300m memory: 256Mi requests: cpu: 300m memory: 256Mi nodeSelector: instance-type: spot This will tell eksctl to : Refer to this one as of kind: Deployment This will have 1 replica with service: nginx and app: nginx Container will use image: nginx and will call it as name: test-autoscaler Specify resources with cpu & memory tags nodeSelector specifies what kind of instances we will need using tag instance-type: spot","title":"Create the deployment file"},{"location":"test-cluster-autoscaler/#deploy-nginx","text":"We will now apply the deployment file using kubectl command kubectl apply -f nginx-deployment.yaml Now check if the pod is running kubectl get pod S C:\\Users\\aniru\\workspace\\github\\aws-eks\\cluster-autoscaler> kubectl get pod NAME READY STATUS RESTARTS AGE test-autoscaler-6545fc8df4-7pktn 0/1 Pending 0 22s check the instance types kubectl get nodes -l instance-type=spot PS C:\\Users\\aniru\\workspace\\github\\aws-eks\\cluster-autoscaler> kubectl get nodes -l instance-type=spot NAME STATUS ROLES AGE VERSION ip-192-168-12-53.ec2.internal Ready <none> 8m5s v1.23.15-eks-49d8fe8","title":"Deploy Nginx"},{"location":"test-cluster-autoscaler/#scale-the-deployment","text":"Let's scale deployment to 3 replicas so we can test the autoscaler kubectl scale --replicas=3 deployment/test-autoscaler S C:\\Users\\aniru\\workspace\\github\\aws-eks\\cluster-autoscaler> kubectl scale --replicas=3 deployment/test-autoscaler deployment.apps/test-autoscaler scaled You should see 3 pods running now but using the same node kubectl get pod S C:\\Users\\aniru\\workspace\\github\\aws-eks\\cluster-autoscaler> kubectl get pod NAME READY STATUS RESTARTS AGE test-autoscaler-6545fc8df4-6qk5f 0/1 Pending 0 33s test-autoscaler-6545fc8df4-7pktn 0/1 Pending 0 6m12s test-autoscaler-6545fc8df4-hmrxd 0/1 Pending 0 33s kubectl get nodes -l instance-type=spot PS C:\\Users\\aniru\\workspace\\github\\aws-eks\\cluster-autoscaler> kubectl get nodes -l instance-type=spot NAME STATUS ROLES AGE VERSION ip-192-168-12-53.ec2.internal Ready <none> 15m v1.23.15-eks-49d8fe8","title":"Scale the deployment"},{"location":"test-cluster-autoscaler/#scale-out-the-deployment","text":"Let's test scale-out by increasing the number of replicas to 4 kubectl scale --replicas=4 deployment/test-autoscaler Now check the pods kubectl get pods -o wide PS C:\\Users\\aniru\\workspace\\github\\aws-eks\\cluster-autoscaler> kubectl get pods -o wide --watch NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-autoscaler-6545fc8df4-6qk5f 0/1 Pending 0 4m39s <none> <none> <none> <none> test-autoscaler-6545fc8df4-7pktn 0/1 Pending 0 10m <none> <none> <none> <none> test-autoscaler-6545fc8df4-hmrxd 0/1 Pending 0 4m39s <none> <none> <none> <none> test-autoscaler-6545fc8df4-kfqg4 0/1 Pending 0 13s <none> <none> <none> <none> And let's check the # of instances. It should increase to accomodate the increased # of replicas kubectl get nodes -l instance-type=spot Now check page Spot Requests on AWS Console after a few minutes","title":"Scale-out the deployment"},{"location":"test-cluster-autoscaler/#scale-in-the-deployment","text":"Let's test scale-out by reducing the number of replicas to 1 kubectl scale --replicas=1 deployment/test-autoscaler Now check the pods kubectl get pod PS C:\\Users\\aniru\\workspace\\github\\aws-eks\\cluster-autoscaler> kubectl scale --replicas=1 deployment/test-autoscaler deployment.apps/test-autoscaler scaled PS C:\\Users\\aniru\\workspace\\github\\aws-eks\\cluster-autoscaler> kubectl get pod NAME READY STATUS RESTARTS AGE test-autoscaler-6545fc8df4-7pktn 0/1 Pending 0 21m And let's check the # of instances. It should increase to accomodate the increased # of replicas kubectl get nodes -l instance-type=spot Now check page EC2 > Instances > Spot Requests on AWS Console after a few minutes. Also check page EC2 > Auto Scaling Groups","title":"Scale-in the deployment"},{"location":"test-cluster-autoscaler/#check-logs","text":"kubectl -n kube-system logs deployment.apps/cluster-autoscaler S C:\\Users\\aniru\\workspace\\github\\aws-eks\\cluster-autoscaler> kubectl -n kube-system logs deployment.apps/cluster-autoscaler Found 2 pods, using pod/cluster-autoscaler-c5bf8cc8-glhlr","title":"Check logs"}]}